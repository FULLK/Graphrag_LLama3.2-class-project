14:41:41,768 asyncio DEBUG Using selector: EpollSelector
14:41:41,779 graphrag.config.read_dotenv INFO Loading pipeline .env file
14:41:41,784 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 6",
        "type": "openai_chat",
        "model": "llama3.2-vision",
        "max_tokens": 500,
        "temperature": 0.6,
        "top_p": 0.9,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:11434/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": ".",
    "reporting": {
        "type": "file",
        "base_dir": "inputs/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "inputs/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_embedding",
            "model": "nomic-embed-text:latest",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 50,
        "overlap": 10,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "llama3.2-vision",
            "max_tokens": 500,
            "temperature": 0.6,
            "top_p": 0.9,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "llama3.2-vision",
            "max_tokens": 500,
            "temperature": 0.6,
            "top_p": 0.9,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "llama3.2-vision",
            "max_tokens": 500,
            "temperature": 0.6,
            "top_p": 0.9,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "llama3.2-vision",
            "max_tokens": 500,
            "temperature": 0.6,
            "top_p": 0.9,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
14:41:41,802 graphrag.index.create_pipeline_config INFO Using LLM Config {
    "api_key": "*****",
    "type": "openai_chat",
    "model": "llama3.2-vision",
    "max_tokens": 500,
    "temperature": 0.6,
    "top_p": 0.9,
    "n": 1,
    "request_timeout": 180.0,
    "api_base": "http://localhost:11434/v1",
    "api_version": null,
    "organization": null,
    "proxy": null,
    "cognitive_services_endpoint": null,
    "deployment_name": null,
    "model_supports_json": true,
    "tokens_per_minute": 0,
    "requests_per_minute": 0,
    "max_retries": 10,
    "max_retry_wait": 10.0,
    "sleep_on_rate_limit_recommendation": true,
    "concurrent_requests": 25
}
14:41:41,803 graphrag.index.create_pipeline_config INFO Using Embeddings Config {
    "api_key": "*****",
    "type": "openai_embedding",
    "model": "nomic-embed-text:latest",
    "max_tokens": 4000,
    "temperature": 0,
    "top_p": 1,
    "n": 1,
    "request_timeout": 180.0,
    "api_base": "http://localhost:11434/v1",
    "api_version": null,
    "organization": null,
    "proxy": null,
    "cognitive_services_endpoint": null,
    "deployment_name": null,
    "model_supports_json": null,
    "tokens_per_minute": 0,
    "requests_per_minute": 0,
    "max_retries": 10,
    "max_retry_wait": 10.0,
    "sleep_on_rate_limit_recommendation": true,
    "concurrent_requests": 25
}
14:41:41,804 graphrag.index.create_pipeline_config INFO skipping workflows 
14:41:41,940 graphrag.index.run INFO Running pipeline
14:41:41,941 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at inputs/artifacts
14:41:41,941 graphrag.index.input.load_input INFO loading input from root_dir=input
14:41:41,941 graphrag.index.input.load_input INFO using file storage for input
14:41:41,943 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
14:41:41,943 graphrag.index.input.text INFO found text files from input, found [('convert.txt', {})]
14:41:41,951 graphrag.index.input.text INFO Found 1 files, loading 1
14:41:41,955 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
14:41:41,955 graphrag.index.run INFO Final # of rows loaded: 1
14:41:42,152 graphrag.index.run INFO Running workflow: create_base_text_units...
14:41:42,152 graphrag.index.run INFO dependencies for create_base_text_units: []
14:41:42,156 datashaper.workflow.workflow INFO executing verb orderby
14:41:42,161 datashaper.workflow.workflow INFO executing verb zip
14:41:42,166 datashaper.workflow.workflow INFO executing verb aggregate_override
14:41:42,174 datashaper.workflow.workflow INFO executing verb chunk
14:41:42,509 datashaper.workflow.workflow INFO executing verb select
14:41:42,515 datashaper.workflow.workflow INFO executing verb unroll
14:41:42,523 datashaper.workflow.workflow INFO executing verb rename
14:41:42,530 datashaper.workflow.workflow INFO executing verb genid
14:41:42,571 datashaper.workflow.workflow INFO executing verb unzip
14:41:42,578 datashaper.workflow.workflow INFO executing verb copy
14:41:42,583 datashaper.workflow.workflow INFO executing verb filter
14:41:42,645 graphrag.index.run DEBUG first row of create_base_text_units => {"id":"bb887bdebdc734a75a534d15740fce82","chunk":"\u5b66\u6821-\u8001\u5e08\t\u8bfe\u7a0b\u540d\u79f0\t\u8bfe\u7a0b\u5185\u5bb9\u4e0e\u8bc4\u4ef7,,,,,,\n\"1001167002-\u4e1c\u5317\u5e08\u8303\u5927\u5b66-\u5c39\u7231\u9752\t\u97f3\u4e50\u8bfe","chunk_id":"bb887bdebdc734a75a534d15740fce82","document_ids":["ea6d05825d08e4710a2d023ea96c1457"],"n_tokens":50}
14:41:42,645 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
14:41:42,881 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
14:41:42,881 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
14:41:42,890 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
14:41:42,927 datashaper.workflow.workflow INFO executing verb entity_extract
14:41:42,928 graphrag.index.verbs.entities.extraction.entity_extract DEBUG entity_extract strategy={'type': "graph_intelligence", 'llm': {'api_key': 'ollama', 'type': "openai_chat", 'model': 'llama3.2-vision', 'max_tokens': 500, 'temperature': 0.6, 'top_p': 0.9, 'n': 1, 'request_timeout': 180.0, 'api_base': 'http://localhost:11434/v1', 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}, 'stagger': 0.3, 'num_threads': 50, 'extraction_prompt': '\n-Goal-\nGiven a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.\nNext, report all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\nFormat each relationship as ("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>)\n\n3. Return output in Chinese as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.\n\n4. If you have to translate into Chinese, just translate the descriptions, nothing else!\n\n5. When finished, output {completion_delimiter}.\n\n-Examples-\n######################\n\nExample 1:\n\ntext:\n�我们一起走进汉语课堂，领略汉语魅力！",,,,,,\n"评价：聆听兰霞老师的每堂讲课，感觉如沐春风，让人如痴如醉， 简直是美伦美奂 精彩绝伦， 扣人心弦 韵味无穷 精彩纷呈 文采飞扬 妙趣横生 辞采华美惟妙惟肖 津津有味 绘声绘色 娓娓动听 妙语连珠 行云流水！使人产生无限的敬佩之情！\n------------------------\noutput:\nBased on the provided text, I will attempt to extract and format the required information according to the specified output format.\n\n**Step 1: Identify Entities**\n\nFrom the given text, I have identified the following entities:\n\n* **Person**: None explicitly mentioned\n* **Location**: None explicitly mentioned\n* **Organization**: None explicitly mentioned\n\nHowever, there are some phrases that could be considered as entities in a broader context:\n* "汉语课堂" (Chinese classroom)\n* "兰霞老师" (Teacher Lan Xia)\n\n**Step 2: Identify Relationships**\n\nFrom the given text, I have identified the following relationships:\n\n* None explicitly mentioned between two entities\n\nHowever, there are some phrases that could be considered as relationships in a broader context:\n* "我们一起走进汉语课堂" (We enter the Chinese classroom together) - This phrase implies a relationship between the speaker and the audience, but it\'s not clear what type of relationship it is.\n* "聆听兰霞老师的每堂讲课" (Listening to Teacher Lan Xia\'s every lecture) - This phrase implies a relationship between the speaker and Teacher Lan Xia, but it\'s not clear what type of relationship it is.\n\n**Step 3: Format Output**\n\nBased on the provided output format, I will attempt to create a structured output. However, since there are no explicit entities or relationships mentioned in the text, the output will be empty.\n\n```\n("entity"{tuple_delimiter}"汉语课堂"{tuple_delimiter}None{tuple_delimiter}"Chinese classroom"{tuple_delimiter}None{tuple_delimiter}8)\n("entity"{tuple_delimiter}"兰霞老师"{tuple_delimiter}None{tuple_delimiter}"Teacher Lan Xia"{tuple_delimiter}None{tuple_delimiter}8)\n\n("relationship"{tuple_delimiter}"我们一起走进汉语课堂"{tuple_delimiter}None{tuple_delimiter}"We enter the Chinese classroom together"{tuple_delimiter}None{tuple_delimiter}8)\n("relationship"{tuple_delimiter}"聆听兰霞老师的每堂讲课"{tuple_delimiter}None{tuple_delimiter}"Listening to Teacher Lan Xia\'s every lecture"{tuple_delimiter}None{tuple_delimiter}8)\n\n```\n\nPlease note that the output is not accurate, as there are no explicit entities or relationships mentioned in the text. The provided output format seems to be designed for a specific type of input data\n#############################\n\n\nExample 2:\n\ntext:\n方案，但是我能认真学习我们的课程，相信对我的教学有新的感悟，创造属于自己的合作课堂。""",,,,,,\n"8009-大连理工大学-章艳\t电子技术（电工学 下）\t""概述：《电工学》课程于2003年被评为辽宁省首批精品课程，2014年被评为辽宁省精品资源共享课程，2018年评为国家精品在线开放课程。课程教学团队为辽宁省教学团队，年均授课学生2000余人。授课教材\n------------------------\noutput:\nBased on the provided text, I will perform the following tasks:\n\n1. Identify entities (people, places, organizations) and their types.\n2. Extract relationships between entities.\n3. Create a list of extracted entities and relationships in the required format.\n\n**Entities:**\n\n1. 人 (person): \n\t* 章艳 (Chapter)\n2. 地点 (location):\n\t* 大连理工大学 (Dalian University of Technology)\n3. 组织 (organization):\n\t* 电子技术（电工学 下）(Electronic Technology, Electrical Engineering)\n4. 课程 (course): \n\t* 《电工学》(Electrical Engineering)\n\n**Relationships:**\n\n1. 章艳是大连理工大学的教员。\n2. 章艳讲授《电工学》课程。\n3. 《电工学》课程被评为辽宁省首批精品课程。\n4. 《电工学》课程被评为辽宁省精品资源共享课程。\n5. 《电工学》课程被评为国家精品在线开放课程。\n\n**Output:**\n\nHere is the list of extracted entities and relationships in the required format:\n\n```\n{\n  "entities": [\n    {\n      "name": "",\n      "type": "person"\n    },\n    {\n      "name": "大连理工大学",\n      "type": "location"\n    },\n    {\n      "name": "电子技术（电工学 下）",\n      "type": "organization"\n    },\n    {\n      "name": "《电工学》",\n      "type": "course"\n    }\n  ],\n  "relationships": [\n    {\n      "entity1": "",\n      "relationship": "是大连理工大学的教员",\n      "entity2": "大连理工大学"\n    },\n    {\n      "entity1": "",\n      "relationship": "讲授《电工学》课程",\n      "entity2": "《电工学》"\n    },\n    {\n      "entity1": "《电工学》",\n      "relationship": "被评为辽宁省首批精品课程",\n      "entity2": ""\n    },\n    {\n      "entity1": "《电工学》",\n      "relationship": "被评为辽宁省精品资源共享课程",\n      "entity2": ""\n\n#############################\n\n\n\n-Real Data-\n######################\ntext: {input_text}\n######################\noutput:\n', 'max_gleanings': 1, 'encoding_name': 'cl100k_base', 'prechunked': True}
14:41:43,57 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:11434/v1
14:41:43,109 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for llama3.2-vision: TPM=0, RPM=0
14:41:43,109 graphrag.index.llm.load_llm INFO create concurrency limiter for llama3.2-vision: 25
