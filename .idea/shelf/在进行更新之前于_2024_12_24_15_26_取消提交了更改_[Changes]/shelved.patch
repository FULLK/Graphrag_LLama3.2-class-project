Index: other/utils/apiTest.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/utils/apiTest.py b/other/utils/apiTest.py
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/utils/apiTest.py	(date 1734766590579)
@@ -0,0 +1,75 @@
+import requests
+import json
+
+url = "http://localhost:8012/v1/chat/completions"
+headers = {"Content-Type": "application/json"}
+
+# 1、测试全局搜索  graphrag-global-search:latest
+global_data = {
+    "model": "graphrag-global-search:latest",
+    "messages": [{"role": "user", "content": "这个故事的首要主题是什么?记住请使用中文进行回答，不要用英文。"}],
+    "temperature": 0.7,
+    # "stream": True,#True or False
+}
+
+# 2、测试本地搜索  graphrag-local-search:latest
+local_data = {
+    "model": "graphrag-local-search:latest",
+    "messages": [{"role": "user", "content": "唐僧是谁，他的主要关系是什么?记住请使用中文进行回答，不要用英文。"}],
+    "temperature": 0.7,
+    # "stream": True,#True or False
+}
+
+# 3、测试全局和本地搜索  full-model:latest
+full_data = {
+    "model": "full-model:latest",
+    "messages": [{"role": "user", "content": "唐僧是谁，他的主要关系是什么?记住请使用中文进行回答，不要用英文。"}],
+    "temperature": 0.7,
+    # "stream": True,#True or False
+}
+
+# 接收非流式输出
+# 1、测试全局搜索  graphrag-global-search:latest
+response = requests.post(url, headers=headers, data=json.dumps(global_data))
+# 2、测试本地搜索  graphrag-local-search:latest
+# response = requests.post(url, headers=headers, data=json.dumps(local_data))
+# 3、测试全局和本地搜索  full-model:latest
+# response = requests.post(url, headers=headers, data=json.dumps(full_data))
+
+# print(response.json())
+print(response.json()['choices'][0]['message']['content'])
+
+
+
+
+
+# # 接收非流式输出
+# try:
+#     with requests.post(url, stream=True, headers=headers, data=json.dumps(data)) as response:
+#         for line in response.iter_lines():
+#         # for line in response.iter_content(chunk_size=16):
+#             if line:
+#                 json_str = line.decode('utf-8').strip("data: ")
+#
+#                 # 检查是否为空或不合法的字符串
+#                 if not json_str:
+#                     print("Received empty string, skipping...")
+#                     continue
+#
+#                 # 确保字符串是有效的JSON格式
+#                 if json_str.startswith('{') and json_str.endswith('}'):
+#                     try:
+#                         data = json.loads(json_str)
+#                         print(f"Received JSON data: {data['choices'][0]['delta']['content']}")
+#                         # print(f"{data['choices'][0]['delta']['content']}")
+#                     except json.JSONDecodeError as e:
+#                         print(f"Failed to decode JSON: {e}")
+#                 else:
+#                     print(f"Invalid JSON format: {json_str}")
+# except Exception as e:
+#     print(f"Error occurred: {e}")
+
+
+
+
+
Index: other/utils/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/utils/main.py b/other/utils/main.py
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/utils/main.py	(date 1734766590579)
@@ -0,0 +1,514 @@
+import os
+import asyncio
+import time
+import uuid
+import json
+import re
+import pandas as pd
+import tiktoken
+import logging
+from fastapi import FastAPI, HTTPException, Request
+from fastapi.responses import JSONResponse, StreamingResponse
+from pydantic import BaseModel, Field
+from typing import List, Optional, Dict, Any, Union
+from contextlib import asynccontextmanager
+import uvicorn
+
+# GraphRAG 相关导入
+from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
+from graphrag.query.indexer_adapters import (
+    read_indexer_covariates,
+    read_indexer_entities,
+    read_indexer_relationships,
+    read_indexer_reports,
+    read_indexer_text_units,
+)
+from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings
+from graphrag.query.llm.oai.chat_openai import ChatOpenAI
+from graphrag.query.llm.oai.embedding import OpenAIEmbedding
+from graphrag.query.llm.oai.typing import OpenaiApiType
+from graphrag.query.question_gen.local_gen import LocalQuestionGen
+from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
+from graphrag.query.structured_search.local_search.search import LocalSearch
+from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext
+from graphrag.query.structured_search.global_search.search import GlobalSearch
+from graphrag.vector_stores.lancedb import LanceDBVectorStore
+
+# 设置日志模版
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+logger = logging.getLogger(__name__)
+
+
+# 设置常量和配置  INPUT_DIR根据自己的建立graphrag的文件夹路径进行修改
+INPUT_DIR = "/Users/janetjiang/Desktop/agi_code/GraphragTest/ragtest/inputs/artifacts"
+LANCEDB_URI = f"{INPUT_DIR}/lancedb"
+COMMUNITY_REPORT_TABLE = "create_final_community_reports"
+ENTITY_TABLE = "create_final_nodes"
+ENTITY_EMBEDDING_TABLE = "create_final_entities"
+RELATIONSHIP_TABLE = "create_final_relationships"
+COVARIATE_TABLE = "create_final_covariates"
+TEXT_UNIT_TABLE = "create_final_text_units"
+COMMUNITY_LEVEL = 2
+PORT = 8012
+
+# 全局变量，用于存储搜索引擎和问题生成器
+local_search_engine = None
+global_search_engine = None
+question_generator = None
+
+
+# 定义Message类型
+class Message(BaseModel):
+    role: str
+    content: str
+
+
+# 定义ChatCompletionRequest类
+class ChatCompletionRequest(BaseModel):
+    model: str
+    messages: List[Message]
+    temperature: Optional[float] = 1.0
+    top_p: Optional[float] = 1.0
+    n: Optional[int] = 1
+    stream: Optional[bool] = False
+    stop: Optional[Union[str, List[str]]] = None
+    max_tokens: Optional[int] = None
+    presence_penalty: Optional[float] = 0
+    frequency_penalty: Optional[float] = 0
+    logit_bias: Optional[Dict[str, float]] = None
+    user: Optional[str] = None
+
+
+# 定义ChatCompletionResponseChoice类
+class ChatCompletionResponseChoice(BaseModel):
+    index: int
+    message: Message
+    finish_reason: Optional[str] = None
+
+
+# 定义Usage类
+class Usage(BaseModel):
+    prompt_tokens: int
+    completion_tokens: int
+    total_tokens: int
+
+
+# 定义ChatCompletionResponse类
+class ChatCompletionResponse(BaseModel):
+    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
+    object: str = "chat.completion"
+    created: int = Field(default_factory=lambda: int(time.time()))
+    model: str
+    choices: List[ChatCompletionResponseChoice]
+    usage: Usage
+    system_fingerprint: Optional[str] = None
+
+
+# 设置语言模型（LLM）、token编码器（TokenEncoder）和文本嵌入向量生成器（TextEmbedder）
+async def setup_llm_and_embedder():
+    logger.info("正在设置LLM和嵌入器")
+    # 实例化一个ChatOpenAI客户端对象
+    llm = ChatOpenAI(
+        # # 调用gpt
+        # api_base="https://api.wlai.vip/v1",  # 请求的API服务地址
+        # api_key="sk-4P8HC2GD6heTwx0l8dD83f13F1014e039eC4Ac6d47877dCb",  # API Key
+        # model="gpt-4o-mini",  # 本次使用的模型
+        # api_type=OpenaiApiType.OpenAI,
+
+        # # 调用其他模型  通过oneAPI
+        # api_base="http://139.224.72.218:3000/v1",  # 请求的API服务地址
+        # api_key="sk-KtEtYw4jOGtSpr4n2e06Ee978690452183Be8a1fF75cA8C5",  # API Key
+        # model="qwen-plus",  # 本次使用的模型
+        # api_type=OpenaiApiType.OpenAI,
+
+        # 调用本地大模型  通过Ollama
+        api_base="http://localhost:11434/v1",  # 请求的API服务地址
+        api_key="ollama",  # API Key
+        model="qwen2:latest",  # 本次使用的模型
+        api_type=OpenaiApiType.OpenAI,
+    )
+
+    # 初始化token编码器
+    token_encoder = tiktoken.get_encoding("cl100k_base")
+
+    # 实例化OpenAIEmbeddings处理模型
+    text_embedder = OpenAIEmbedding(
+        # # 调用gpt
+        # api_base="https://api.wlai.vip/v1",  # 请求的API服务地址
+        # api_key="sk-Soz7kmey8JKidej0AeD416B87d2547E1861d29F4F3E7A75e",  # API Key
+        # model="text-embedding-3-small",
+        # deployment_name="text-embedding-3-small",
+        # api_type=OpenaiApiType.OpenAI,
+        # max_retries=20,
+
+        # # 调用其他模型  通过oneAPI
+        # api_base="http://139.224.72.218:3000/v1",  # 请求的API服务地址
+        # api_key="sk-KtEtYw4jOGtSpr4n2e06Ee978690452183Be8a1fF75cA8C5",  # API Key
+        # model="text-embedding-v1",
+        # deployment_name="text-embedding-v1",
+        # api_type=OpenaiApiType.OpenAI,
+        # max_retries=20,
+
+        # 调用本地大模型  通过Ollama
+        api_base="http://localhost:11434/v1",  # 请求的API服务地址
+        api_key="ollama",  # API Key
+        model="nomic-embed-text:latest",
+        deployment_name="nomic-embed-text:latest",
+        api_type=OpenaiApiType.OpenAI,
+        max_retries=20,
+        
+    )
+
+    logger.info("LLM和嵌入器设置完成")
+    return llm, token_encoder, text_embedder
+
+
+# 加载上下文数据，包括实体、关系、报告、文本单元和协变量
+async def load_context():
+    logger.info("正在加载上下文数据")
+    try:
+        # 使用pandas库从指定的路径读取实体数据表ENTITY_TABLE，文件格式为Parquet，并将其加载为DataFrame，存储在变量entity_df中
+        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
+        # 读取实体嵌入向量数据表ENTITY_EMBEDDING_TABLE，并将其加载为DataFrame，存储在变量entity_embedding_df中
+        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
+        # 将entity_df和entity_embedding_df传入，并基于COMMUNITY_LEVEL（社区级别）处理这些数据，返回处理后的实体数据entities
+        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
+        # 创建一个LanceDBVectorStore的实例description_embedding_store，用于存储实体的描述嵌入向量
+        # 这个实例与一个名为"entity_description_embeddings_xiyoujiqwen"的集合（collection）相关联
+        description_embedding_store = LanceDBVectorStore(collection_name="entity_description_embeddings")
+        # 通过调用connect方法，连接到指定的LanceDB数据库，使用的URI存储在LANCEDB_URI变量中
+        description_embedding_store.connect(db_uri=LANCEDB_URI)
+        # 将已处理的实体数据entities存储到description_embedding_store中，用于语义搜索或其他用途
+        store_entity_semantic_embeddings(entities=entities, vectorstore=description_embedding_store)
+        relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
+        relationships = read_indexer_relationships(relationship_df)
+        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
+        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
+        text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
+        text_units = read_indexer_text_units(text_unit_df)
+        covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
+        claims = read_indexer_covariates(covariate_df)
+        logger.info(f"声明记录数: {len(claims)}")
+        covariates = {"claims": claims}
+        logger.info("上下文数据加载完成")
+        return entities, relationships, reports, text_units, description_embedding_store, covariates
+    except Exception as e:
+        logger.error(f"加载上下文数据时出错: {str(e)}")
+        raise
+
+
+# 设置本地和全局搜索引擎、上下文构建器（ContextBuilder）、以及相关参数
+async def setup_search_engines(llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
+                               description_embedding_store, covariates):
+    logger.info("正在设置搜索引擎")
+    # 设置本地搜索引擎
+    local_context_builder = LocalSearchMixedContext(
+        community_reports=reports,
+        text_units=text_units,
+        entities=entities,
+        relationships=relationships,
+        covariates=covariates,
+        entity_text_embeddings=description_embedding_store,
+        embedding_vectorstore_key=EntityVectorStoreKey.ID,
+        text_embedder=text_embedder,
+        token_encoder=token_encoder,
+    )
+
+    local_context_params = {
+        "text_unit_prop": 0.5,
+        "community_prop": 0.1,
+        "conversation_history_max_turns": 5,
+        "conversation_history_user_turns_only": True,
+        "top_k_mapped_entities": 10,
+        "top_k_relationships": 10,
+        "include_entity_rank": True,
+        "include_relationship_weight": True,
+        "include_community_rank": False,
+        "return_candidate_context": False,
+        "embedding_vectorstore_key": EntityVectorStoreKey.ID,
+        # "max_tokens": 12_000,
+        "max_tokens": 4096,
+    }
+
+    local_llm_params = {
+        # "max_tokens": 2_000,
+        "max_tokens": 4096,
+        "temperature": 0.0,
+    }
+
+    local_search_engine = LocalSearch(
+        llm=llm,
+        context_builder=local_context_builder,
+        token_encoder=token_encoder,
+        llm_params=local_llm_params,
+        context_builder_params=local_context_params,
+        response_type="multiple paragraphs",
+    )
+
+    # 设置全局搜索引擎
+    global_context_builder = GlobalCommunityContext(
+        community_reports=reports,
+        entities=entities,
+        token_encoder=token_encoder,
+    )
+
+    global_context_builder_params = {
+        "use_community_summary": False,
+        "shuffle_data": True,
+        "include_community_rank": True,
+        "min_community_rank": 0,
+        "community_rank_name": "rank",
+        "include_community_weight": True,
+        "community_weight_name": "occurrence weight",
+        "normalize_community_weight": True,
+        # "max_tokens": 12_000,
+        "max_tokens": 4096,
+        "context_name": "Reports",
+    }
+
+    map_llm_params = {
+        "max_tokens": 1000,
+        "temperature": 0.0,
+        "response_format": {"type": "json_object"},
+    }
+
+    reduce_llm_params = {
+        "max_tokens": 2000,
+        "temperature": 0.0,
+    }
+
+    global_search_engine = GlobalSearch(
+        llm=llm,
+        context_builder=global_context_builder,
+        token_encoder=token_encoder,
+        # max_data_tokens=12_000,
+        max_data_tokens=4096,
+        map_llm_params=map_llm_params,
+        reduce_llm_params=reduce_llm_params,
+        allow_general_knowledge=False,
+        json_mode=True,
+        context_builder_params=global_context_builder_params,
+        concurrent_coroutines=32,
+        response_type="multiple paragraphs",
+    )
+
+    logger.info("搜索引擎设置完成")
+    return local_search_engine, global_search_engine, local_context_builder, local_llm_params, local_context_params
+
+
+# 格式化响应，对输入的文本进行段落分隔、添加适当的换行符，以及在代码块中增加标记，以便生成更具可读性的输出
+def format_response(response):
+    # 使用正则表达式 \n{2, }将输入的response按照两个或更多的连续换行符进行分割。这样可以将文本分割成多个段落，每个段落由连续的非空行组成
+    paragraphs = re.split(r'\n{2,}', response)
+    # 空列表，用于存储格式化后的段落
+    formatted_paragraphs = []
+    # 遍历每个段落进行处理
+    for para in paragraphs:
+        # 检查段落中是否包含代码块标记
+        if '```' in para:
+            # 将段落按照```分割成多个部分，代码块和普通文本交替出现
+            parts = para.split('```')
+            for i, part in enumerate(parts):
+                # 检查当前部分的索引是否为奇数，奇数部分代表代码块
+                if i % 2 == 1:  # 这是代码块
+                    # 将代码块部分用换行符和```包围，并去除多余的空白字符
+                    parts[i] = f"\n```\n{part.strip()}\n```\n"
+            # 将分割后的部分重新组合成一个字符串
+            para = ''.join(parts)
+        else:
+            # 否则，将句子中的句点后面的空格替换为换行符，以便句子之间有明确的分隔
+            para = para.replace('. ', '.\n')
+        # 将格式化后的段落添加到formatted_paragraphs列表
+        # strip()方法用于移除字符串开头和结尾的空白字符（包括空格、制表符 \t、换行符 \n等）
+        formatted_paragraphs.append(para.strip())
+    # 将所有格式化后的段落用两个换行符连接起来，以形成一个具有清晰段落分隔的文本
+    return '\n\n'.join(formatted_paragraphs)
+
+
+# 定义了一个异步函数 lifespan，它接收一个 FastAPI 应用实例 app 作为参数。这个函数将管理应用的生命周期，包括启动和关闭时的操作
+# 函数在应用启动时执行一些初始化操作，如设置搜索引擎、加载上下文数据、以及初始化问题生成器
+# 函数在应用关闭时执行一些清理操作
+# @asynccontextmanager 装饰器用于创建一个异步上下文管理器，它允许你在 yield 之前和之后执行特定的代码块，分别表示启动和关闭时的操作
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    # 启动时执行
+    # 申明引用全局变量，在函数中被初始化，并在整个应用中使用
+    global local_search_engine, global_search_engine, question_generator
+    try:
+        logger.info("正在初始化搜索引擎和问题生成器...")
+        # 调用setup_llm_and_embedder()函数以设置语言模型（LLM）、token编码器（TokenEncoder）和文本嵌入向量生成器（TextEmbedder）
+        # await 关键字表示此调用是异步的，函数将在这个操作完成后继续执行
+        llm, token_encoder, text_embedder = await setup_llm_and_embedder()
+        # 调用load_context()函数加载实体、关系、报告、文本单元、描述嵌入存储和协变量等数据，这些数据将用于构建搜索引擎和问题生成器
+        entities, relationships, reports, text_units, description_embedding_store, covariates = await load_context()
+        # 调用setup_search_engines()函数设置本地和全局搜索引擎、上下文构建器（ContextBuilder）、以及相关参数
+        local_search_engine, global_search_engine, local_context_builder, local_llm_params, local_context_params = await setup_search_engines(
+            llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
+            description_embedding_store, covariates
+        )
+        # 使用LocalQuestionGen类创建一个本地问题生成器question_generator，将前面初始化的各种组件传递给它
+        question_generator = LocalQuestionGen(
+            llm=llm,
+            context_builder=local_context_builder,
+            token_encoder=token_encoder,
+            llm_params=local_llm_params,
+            context_builder_params=local_context_params,
+        )
+        logger.info("初始化完成")
+    except Exception as e:
+        logger.error(f"初始化过程中出错: {str(e)}")
+        # raise 关键字重新抛出异常，以确保程序不会在错误状态下继续运行
+        raise
+    # yield 关键字将控制权交还给FastAPI框架，使应用开始运行
+    # 分隔了启动和关闭的逻辑。在yield 之前的代码在应用启动时运行，yield 之后的代码在应用关闭时运行
+    yield
+
+    # 关闭时执行
+    logger.info("正在关闭...")
+
+
+# lifespan 参数用于在应用程序生命周期的开始和结束时执行一些初始化或清理工作
+app = FastAPI(lifespan=lifespan)
+
+
+# 执行全模型搜索，包括本地检索、全局检索
+async def full_model_search(prompt: str):
+    local_result = await local_search_engine.asearch(prompt)
+    global_result = await global_search_engine.asearch(prompt)
+    # 格式化结果
+    formatted_result = "#综合搜索结果:\n\n"
+    formatted_result += "##本地检索结果:\n"
+    formatted_result += format_response(local_result.response) + "\n\n"
+    formatted_result += "##全局检索结果:\n"
+    formatted_result += format_response(global_result.response) + "\n\n"
+    return formatted_result
+
+
+# POST请求接口，与大模型进行知识问答
+@app.post("/v1/chat/completions")
+async def chat_completions(request: ChatCompletionRequest):
+    if not local_search_engine or not global_search_engine:
+        logger.error("搜索引擎未初始化")
+        raise HTTPException(status_code=500, detail="搜索引擎未初始化")
+
+    try:
+        logger.info(f"收到聊天完成请求: {request}")
+        prompt = request.messages[-1].content
+        logger.info(f"处理提示: {prompt}")
+
+        # 根据模型选择使用不同的搜索方法
+        if request.model == "graphrag-global-search:latest":
+            result = await global_search_engine.asearch(prompt)
+            formatted_response = format_response(result.response)
+        elif request.model == "full-model:latest":
+            formatted_response = await full_model_search(prompt)
+        elif request.model == "graphrag-local-search:latest":  # 默认使用本地搜索
+            result = await local_search_engine.asearch(prompt)
+            formatted_response = format_response(result.response)
+
+        logger.info(f"格式化的搜索结果:\n {formatted_response}")
+
+        # 流式响应和非流式响应的处理保持不变
+        if request.stream:
+            # 定义一个异步生成器函数，用于生成流式数据
+            async def generate_stream():
+                # 为每个流式数据片段生成一个唯一的chunk_id
+                chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
+                # 将格式化后的响应按行分割
+                lines = formatted_response.split('\n')
+                # 历每一行，并构建响应片段
+                for i, line in enumerate(lines):
+                    # 创建一个字典，表示流式数据的一个片段
+                    chunk = {
+                        "id": chunk_id,
+                        "object": "chat.completion.chunk",
+                        "created": int(time.time()),
+                        "model": request.model,
+                        "choices": [
+                            {
+                                "index": 0,
+                                "delta": {"content": line + '\n'}, # if i > 0 else {"role": "assistant", "content": ""},
+                                "finish_reason": None
+                            }
+                        ]
+                    }
+                    # 将片段转换为JSON格式并生成
+                    yield f"data: {json.dumps(chunk)}\n"
+                    # 每次生成数据后，异步等待0.5秒
+                    await asyncio.sleep(0.5)
+                # 生成最后一个片段，表示流式响应的结束
+                final_chunk = {
+                    "id": chunk_id,
+                    "object": "chat.completion.chunk",
+                    "created": int(time.time()),
+                    "model": request.model,
+                    "choices": [
+                        {
+                            "index": 0,
+                            "delta": {},
+                            "finish_reason": "stop"
+                        }
+                    ]
+                }
+                yield f"data: {json.dumps(final_chunk)}\n"
+                yield "data: [DONE]\n"
+
+            # 返回StreamingResponse对象，流式传输数据，media_type设置为text/event-stream以符合SSE(Server-SentEvents) 格式
+            return StreamingResponse(generate_stream(), media_type="text/event-stream")
+        # 非流式响应处理
+        else:
+            response = ChatCompletionResponse(
+                model=request.model,
+                choices=[
+                    ChatCompletionResponseChoice(
+                        index=0,
+                        message=Message(role="assistant", content=formatted_response),
+                        finish_reason="stop"
+                    )
+                ],
+                # 使用情况
+                usage=Usage(
+                    # 提示文本的tokens数量
+                    prompt_tokens=len(prompt.split()),
+                    # 完成文本的tokens数量
+                    completion_tokens=len(formatted_response.split()),
+                    # 总tokens数量
+                    total_tokens=len(prompt.split()) + len(formatted_response.split())
+                )
+            )
+            logger.info(f"发送响应: \n\n{response}")
+            # 返回JSONResponse对象，其中content是将response对象转换为字典的结果
+            return JSONResponse(content=response.dict())
+
+    except Exception as e:
+        logger.error(f"处理聊天完成时出错:\n\n {str(e)}")
+        raise HTTPException(status_code=500, detail=str(e))
+
+
+# GET请求接口，获取可用模型列表
+@app.get("/v1/models")
+async def list_models():
+    logger.info("收到模型列表请求")
+    current_time = int(time.time())
+    models = [
+        {"id": "graphrag-local-search:latest", "object": "model", "created": current_time - 100000, "owned_by": "graphrag"},
+        {"id": "graphrag-global-search:latest", "object": "model", "created": current_time - 95000, "owned_by": "graphrag"},
+        {"id": "full-model:latest", "object": "model", "created": current_time - 80000, "owned_by": "combined"}
+    ]
+
+    response = {
+        "object": "list",
+        "data": models
+    }
+
+    logger.info(f"发送模型列表: {response}")
+    return JSONResponse(content=response)
+
+
+
+if __name__ == "__main__":
+    logger.info(f"在端口 {PORT} 上启动服务器")
+    # uvicorn是一个用于运行ASGI应用的轻量级、超快速的ASGI服务器实现
+    # 用于部署基于FastAPI框架的异步PythonWeb应用程序
+    uvicorn.run(app, host="0.0.0.0", port=PORT)
+
Index: other/utils/spider.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/utils/spider.py b/other/utils/spider.py
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/utils/spider.py	(date 1734766590579)
@@ -0,0 +1,108 @@
+# pip install scrapy html2text bs4
+
+import scrapy
+from scrapy.crawler import CrawlerProcess
+from bs4 import BeautifulSoup
+import html2text
+import os
+import json
+from urllib.parse import urlparse
+
+
+# 功能：爬取网页并将内容转换为 Markdown 格式
+# 自定义的Scrapy爬虫类ContentFocusedSpider，继承自Scrapy的Spider基类
+class ContentFocusedSpider(scrapy.Spider):
+    # name属性定义了这个爬虫的名称，用于标识爬虫
+    name = 'content_focused_spider'
+    # start_urls是爬虫开始抓取的初始URL列表。这里设置了一个初始URL
+    start_urls = ['https://crawl4ai.com/mkdocs/']
+    # allowed_domains指定了爬虫可以抓取的域名。此设置可以防止爬虫越界抓取其他域名的网站内容
+    allowed_domains = ['crawl4ai.com']
+
+    # 初始化方法，用于设置爬虫在启动时的各种配置
+    def __init__(self, *args, **kwargs):
+        super(ContentFocusedSpider, self).__init__(*args, **kwargs)
+        # 创建了一个HTML2Text对象，html2text是一个将HTML转换为Markdown的库
+        self.h = html2text.HTML2Text()
+        # 设置HTML2Text的选项，如忽略链接、忽略图像、忽略强调（如斜体字）等
+        self.h.ignore_links = True
+        self.h.ignore_images = True
+        self.h.ignore_emphasis = True
+        self.h.body_width = 0
+        # 初始化一个空列表self.results，用于存储每个页面的爬取结果）
+        self.results = []
+        # 创建保存数据的目录，如果目录不存在则创建
+        os.makedirs('.data', exist_ok=True)
+        os.makedirs('.data/markdown_files', exist_ok=True)
+
+    # Scrapy中的一个默认解析方法，用于处理每个响应（网页内容）
+    def parse(self, response):
+        # 使用BeautifulSoup解析网页内容为一个soup对象，以便于进一步的HTML处理
+        soup = BeautifulSoup(response.text, 'html.parser')
+        # 移除导航栏、侧边栏、页脚等元素，专注于页面的主要内容
+        for elem in soup(['nav', 'header', 'footer', 'aside']):
+            elem.decompose()
+
+        # 尝试找到主要内容区域，优先查找 <main>、 <article> 或带有content类的 <div> 标签
+        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
+
+        if main_content:
+            content = str(main_content)
+        else:
+            content = str(soup.body)  # 如果找不到明确的主要内容，使用整个 body
+
+        # 使用HTML2Text将提取的HTML内容转换为Markdown格式
+        markdown_content = self.h.handle(content)
+
+        # 保存生成文件名，用于保存Markdown文件。将URL的路径部分转换为文件名，若路径为空则使用'index'
+        parsed_url = urlparse(response.url)
+        file_path = parsed_url.path.strip('/').replace('/', '_') or 'index'
+        # 生成完整的Markdown文件路径
+        markdown_filename = f'.data/markdown_files/{file_path}.md'
+
+        # 打开文件并写入转换后的Markdown内容
+        with open(markdown_filename, 'w', encoding='utf-8') as f:
+            f.write(markdown_content)
+        # 创建一个字典对象result，包含当前页面的URL和保存的Markdown文件路径
+        result = {
+            'url': response.url,
+            'markdown_file': markdown_filename,
+        }
+        # 将当前页面的结果添加到self.results列表中
+        self.results.append(result)
+
+        # 遍历页面中所有的链接并继续爬取这些链接指向的页面
+        # 对每个找到的链接，继续递归爬取，调用self.parse方法处理新的响应
+        for link in response.css('a::attr(href)').getall():
+            yield response.follow(link, self.parse)
+
+    # closed方法在爬虫结束时调用，用于执行清理或保存最终结果的操作
+    def closed(self, reason):
+        # 将所有页面的爬取结果（URL和文件路径）保存到一个JSON文件中
+        with open('.data/markdown_results.json', 'w', encoding='utf-8') as f:
+            json.dump(self.results, f, ensure_ascii=False, indent=2)
+
+        print(f"爬取完成。总共爬取了 {len(self.results)} 个页面")
+        print("结果元数据保存在 .data/markdown_results.json")
+        print("Markdown 文件保存在 .data/markdown_files/ 目录下")
+
+
+
+# CrawlerProcess 是 Scrapy 提供的一个类，用于在脚本中创建并启动爬虫。settings 参数接受一个字典，用于配置爬虫的各种设置
+# USER_AGENT，这个参数指定了爬虫在向服务器发送请求时使用的浏览器标识（即用户代理）。这里的值模仿了一个真实的浏览器，以避免被服务器拒绝访问
+# ROBOTSTXT_OBEY,是否遵守 robots.txt 文件中的爬取规则。True 表示爬虫会遵守这些规则，以避免访问网站管理员不希望爬取的部分
+# CONCURRENT_REQUESTS，定义了同时进行的请求数。在这里被设置为 1，表示一次只发送一个请求，这对于避免服务器过载以及防止被封禁非常重要
+# DOWNLOAD_DELAY，设置了每个请求之间的延迟时间（以秒为单位）。在此处设置为 2 秒，用于减缓爬取速度，避免给目标服务器造成过大的压力
+process = CrawlerProcess(settings={
+    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
+    'ROBOTSTXT_OBEY': True,
+    'CONCURRENT_REQUESTS': 1,
+    'DOWNLOAD_DELAY': 2,
+})
+
+
+# process.crawl 方法启动指定的爬虫类
+# ContentFocusedSpider是定义的一个爬虫类。它包含了爬取逻辑，包括如何处理响应以及从页面提取数据
+process.crawl(ContentFocusedSpider)
+# 启动并开始执行爬取操作。这个方法会阻塞当前脚本，直到所有爬虫任务完成
+process.start()
\ No newline at end of file
Index: other/utils/neo4jTest.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/utils/neo4jTest.py b/other/utils/neo4jTest.py
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/utils/neo4jTest.py	(date 1734766590579)
@@ -0,0 +1,294 @@
+# 1、NEO4J数据库相关
+# NEO4J_URI=neo4j+s://a1daa8d4.databases.neo4j.io
+# NEO4J_USERNAME=neo4j
+# NEO4J_PASSWORD=#12345678
+# AURA_INSTANCEID=a1daa8d4
+# AURA_INSTANCENAME=Instance01
+
+# 2、安装必要的依赖包
+# pip install pandas neo4j-rust-ext
+
+# 3、NEO4J数据库查询使用方法
+# （1）Show a few __Entity__ nodes and their relationships (Entity Graph)
+# MATCH path = (:__Entity__)-[:RELATED]->(:__Entity__)
+# RETURN path LIMIT 200
+# （2）Show the Chunks and the Document (Lexical Graph)
+# MATCH (d:__Document__) WITH d LIMIT 1
+# MATCH path = (d)<-[:PART_OF]-(c:__Chunk__)
+# RETURN path LIMIT 100
+# （3）Show a Community and it's Entities
+# MATCH (c:__Community__) WITH c LIMIT 1
+# MATCH path = (c)<-[:IN_COMMUNITY]-()-[:RELATED]-(:__Entity__)
+# RETURN path LIMIT 100
+# (4) 清除数据
+# MATCH (n)
+# CALL { WITH n DETACH DELETE n } IN TRANSACTIONS OF 25000 ROWS;
+
+# 4、节点间相关描述
+# （1）节点包括:
+# 原始文档(__Document__)
+# 文本块(__Chunk__)
+# 实体(__Entity__,又可分为不同类型)
+# 社区(__Community__)
+# 协变量(__Covariate__)
+
+# （2）关系包括:
+# RELATED(entity之间)
+# PART_OF(chunk与document之间)
+# HAS_ENTITY(chunk与entity之间)
+# IN_COMMUNITY(entity与community之间)
+# HAS_FINDING
+# HAS_COVARIATE(chunk与covariate之间)
+
+
+# 导入相关的包
+import pandas as pd
+from neo4j import GraphDatabase
+import time
+import json
+
+
+# 指定Parquet文件路径
+GRAPHRAG_FOLDER="/Users/janetjiang/Desktop/agi_code/GraphragTest/ragtest/inputs/artifacts"
+
+
+# 数据库连接相关参数配置
+NEO4J_URI="neo4j+s://a1daa8d4.databases.neo4j.io:7687"
+NEO4J_USERNAME="neo4j"
+NEO4J_PASSWORD="#Zzy1234567890"
+NEO4J_DATABASE="neo4j"
+
+
+# 实例化一个图数据库实例
+driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))
+
+
+# 在图数据库中创建约束  初始化
+statements = """
+create constraint document_id if not exists for (d:__Document__) require d.id is unique;
+create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique;
+create constraint entity_id if not exists for (e:__Entity__) require e.id is unique;
+create constraint entity_title if not exists for (e:__Entity__) require e.name is unique;
+create constraint entity_id if not exists for (c:__Community__) require c.community is unique;
+create constraint entity_title if not exists for (e:__Covariate__) require e.title is unique;
+create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique;
+""".split(";")
+for statement in statements:
+    if len((statement or "").strip()) > 0:
+        print(statement)
+        driver.execute_query(statement)
+
+
+# 使用批处理方式将数据帧导入Neo4j
+# 参数：statement 是要执行的 Cypher 查询，df 是要导入的数据帧，batch_size 是每批要导入的行数
+def batched_import(statement, df, batch_size=1000):
+    # 计算数据帧df中的总行数，并将其存储在total变量中
+    total = len(df)
+    # 记录当前时间，以便后续计算导入操作所花费的总时间
+    start_s = time.time()
+    # 每次循环处理一批数据，步数为batch_size
+    for start in range(0,total, batch_size):
+        # 使用Pandas的iloc方法提取当前批次的数据子集
+        # start是当前批次的起始行号
+        # min(start + batch_size, total)是当前批次的结束行号，确保不会超过总行数
+        batch = df.iloc[start: min(start+batch_size,total)]
+        # "UNWIND $rows AS value "是Cypher中的一个操作，它将 $row中的每个元素逐个解包，并作为value传递给Cypher语句statement
+        result = driver.execute_query("UNWIND $rows AS value " + statement,
+                                      # 将当前批次的 DataFrame 转换为字典的列表
+                                      # 每一行数据变成一个字典，columns 作为键
+                                      rows=batch.to_dict('records'),
+                                      database_=NEO4J_DATABASE)
+        # 打印执行结果的摘要统计信息，包括创建的节点、关系等计数
+        print(result.summary.counters)
+    # 计算并打印导入总行数和耗时
+    print(f'{total} rows in { time.time() - start_s} s.')
+    # 返回导入的总行数
+    return total
+
+
+# 按顺序依次执行如下步骤
+
+# 1、创建或更新documents
+# 从指定的 Parquet 文件 create_final_documents.parquet 中读取 id、title 和 raw_content 这三列
+# 并将它们加载到一个名为 doc_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+doc_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_documents.parquet', columns=["id", "title", "raw_content"])
+# # 打印输出数据帧 doc_df 的前 30 行内容
+# print(doc_df.head(30))
+# MERGE (d:__Document__ {id:value.id})尝试在数据库中找到一个具有 id 属性值为 value.id 的 __Document__ 节点
+# 如果找到，则匹配这个节点；如果找不到，则创建一个新的 __Document__ 节点，并将 id 属性设置为 value.id
+# SET d += value {.title, .raw_content}将外部 value 对象的 title 属性值赋给节点 d 的 title 属性
+# 如果 d 节点已经存在 title 属性，它将被更新为新值；如果 d 节点没有 title 属性，则会新建一个
+statement = """
+MERGE (d:__Document__ {id:value.id})
+SET d += value {.title, .raw_content}
+"""
+total = batched_import(statement, doc_df)
+print("返回的结果：",total)
+
+
+# 2、创建或更新chunks与documents之间的关系
+# 从指定的 Parquet 文件 create_final_text_units.parquet 中读取列
+# 并将它们加载到一个名为 text_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+text_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_text_units.parquet',
+                          columns=["id","text","n_tokens","document_ids","entity_ids","relationship_ids","covariate_ids"])
+# 打印输出数据帧 text_df 的前 30 行内容
+print(text_df.head(30))
+# MERGE (c:__Chunk__ {id:value.id})尝试匹配或创建一个节点。如果具有指定属性的节点存在，则返回该节点；否则，创建一个新的节点
+# SET c += value {.text, .n_tokens}从 value 对象中提取属性，并将它们赋值给节点 c 的同名属性
+# WITH c, value用于将当前查询上下文中的变量传递给接下来的查询部分。在这里，c 和 value 被传递到下一步的查询中
+# UNWIND value.document_ids AS document将列表 value.document_ids 中的每个元素依次展开为单独的记录,并将每个元素命名为 document，进行单独处理
+# MATCH (d:__Document__ {id:document})查找 __Document__ 标签的节点，并且 id 属性值等于 document
+# MERGE (c)-[:PART_OF]->(d)在__Chunk__节点与 __Document__ 节点之间创建一个 PART_OF 类型的关系。如果关系已经存在，则不创建新的关系，表示 c 是 d 的一部分
+statement = """
+MERGE (c:__Chunk__ {id:value.id})
+SET c += value {.text, .n_tokens}
+WITH c, value
+UNWIND value.document_ids AS document
+MATCH (d:__Document__ {id:document})
+MERGE (c)-[:PART_OF]->(d)
+"""
+batched_import(statement, text_df)
+
+
+# 3、创建或更新entities与chunks之间的关系
+# 从指定的 Parquet 文件 create_final_entities.parquet 中读取列
+# 并将它们加载到一个名为 entity_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+entity_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_entities.parquet',
+                            columns=["name","type","description","human_readable_id","id","description_embedding","text_unit_ids"])
+# 打印输出数据帧 entity_df 的前 30 行内容
+print(entity_df.head(30))
+# MERGE (e:__Entity__ {id:value.id})尝试匹配或创建一个节点。如果具有指定属性的节点存在，则返回该节点；否则，创建一个新的节点
+# SET e += value {.name, .type, .description, .human_readable_id, .id, .description_embedding, .text_unit_ids}从 value 对象中提取属性，并将它们赋值给节点 e 的同名属性
+# WITH e, value用于将当前查询上下文中的变量传递给接下来的查询部分。在这里，e 和 value 被传递到下一步的查询中
+# CALL db.create.setNodeVectorProperty(e, "description_embedding", value.description_embedding)调用 Neo4j 中的自定义过程，设置 e 节点的 description_embedding 属性,将 value.description_embedding 的值作为向量属性存储在 e 节点的 description_embedding 属性中
+# CALL apoc.create.addLabels()使用 APOC 库中的方法为节点 e 添加标签,根据 value.type 的值决定要添加的标签即将entity的类型均设置为标签
+# UNWIND value.text_unit_ids AS text_unit将列表 value.text_unit_ids 中的每个元素依次展开为单独的记录,并将每个元素命名为 text_unit，进行单独处理
+# MATCH (c:__Chunk__ {id:text_unit})查找 __Chunk__ 标签的节点，并且 id 属性值等于 text_unit
+# MERGE (c)-[:HAS_ENTITY]->(e)在__Chunk__节点与 __Entity__ 节点之间创建一个 HAS_ENTITY 类型的关系。如果关系已经存在，则不创建新的关系，表示表示该文本块包含该实体
+entity_statement = """
+MERGE (e:__Entity__ {id:value.id})
+SET e += value {.name, .type, .description, .human_readable_id, .id, .description_embedding, .text_unit_ids}
+WITH e, value
+CALL db.create.setNodeVectorProperty(e, "description_embedding", value.description_embedding)
+CALL apoc.create.addLabels(e, case when coalesce(value.type,"") = "" then [] else [apoc.text.upperCamelCase(replace(value.type,'"',''))] end) yield node
+UNWIND value.text_unit_ids AS text_unit
+MATCH (c:__Chunk__ {id:text_unit})
+MERGE (c)-[:HAS_ENTITY]->(e)
+"""
+batched_import(entity_statement, entity_df)
+
+
+# 4、创建或更新entity节点之间的关系
+# 从指定的 Parquet 文件 create_final_relationships.parquet 中读取列
+# 并将它们加载到一个名为 rel_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+rel_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_relationships.parquet',
+                         columns=["source","target","id","rank","weight","human_readable_id","description","text_unit_ids"])
+# 打印输出数据帧 rel_df 的前 30 行内容
+print(rel_df.head(30))
+# MATCH (source:__Entity__ {name:replace(value.source,'"','')})查找 __Entity__ 标签的节点，并且 name 属性等于 value.source 中的内容（其中所有双引号 " 被替换为空字符串）。找到的节点赋值给变量 source
+# MATCH (target:__Entity__ {name:replace(value.target,'"','')})查找 __Entity__ 标签的节点，并且 name 属性等于 value.target 中的内容（其中所有双引号 " 被替换为空字符串）。找到的节点赋值给变量 target
+# MERGE (source)-[rel:RELATED {id: value.id}]->(target)在 source 和 target 之间查找或创建一个 RELATED 类型的关系，并为该关系设置 id 属性。如果已经存在具有相同 id 的关系，则更新其属性
+# SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}将 value 对象中的 rank、weight、human_readable_id、description 和 text_unit_ids 属性合并到该关系上
+# RETURN count(*) as createdRels返回创建或更新的关系数量，并将结果命名为 createdRels
+rel_statement = """
+    MATCH (source:__Entity__ {name:replace(value.source,'"','')})
+    MATCH (target:__Entity__ {name:replace(value.target,'"','')})
+    MERGE (source)-[rel:RELATED {id: value.id}]->(target)
+    SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}
+    RETURN count(*) as createdRels
+"""
+batched_import(rel_statement, rel_df)
+
+
+# 5、创建或更新community与entity、chunk节点之间的关系
+# 从指定的 Parquet 文件 create_final_community_reports.parquet 中读取列
+# 并将它们加载到一个名为 community_report_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+community_report_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_community_reports.parquet',
+                               columns=["id","community","findings","title","summary", "level","rank","rank_explanation","full_content"])
+# 打印输出数据帧 rel_df 的前 30 行内容
+print(community_report_df.head(30))
+# MERGE (c:__Community__ {id:value.id})查找或创建一个 __Community__ 节点，并设置其 id 属性为 value.id 的值。如果具有相同 id 的节点已存在，则返回该节点；否则，创建新的节点
+# SET c += value {.community, .level,}将 value 对象中指定的属性和值合并到节点 c 上，不覆盖现有属性。从 value 对象中提取属性，并赋值给 c 节点的同名属性
+# WITH c, value将当前查询上下文中的 c 和 value 变量传递给接下来的查询部分
+# UNWIND：将列表展开为多个行，生成一个从 0 到 value.findings 列表大小减一的范围列表，表示所有 finding 项的索引，将每个索引值赋值给 finding_idx 变量
+# WITH c, value, finding_idx, value.findings[finding_idx] as finding将当前查询上下文中的变量传递到下一个查询部分
+# MERGE (c)-[:HAS_FINDING]->(f:Finding {id:finding_idx})查找或创建 __Community__ 节点 c 与 Finding 节点 f 之间的 HAS_FINDING 关系。Finding 节点的 id 属性设置为 finding_idx
+# SET f += finding将 finding 对象中的属性和值合并到 f 节点上
+community_statement = """
+MERGE (c:__Community__ {id:value.id})
+SET c += value {.community, .level, .title, .rank, .rank_explanation, .full_content, .summary}
+WITH c, value
+UNWIND range(0, size(value.findings)-1) AS finding_idx
+WITH c, value, finding_idx, value.findings[finding_idx] as finding
+MERGE (c)-[:HAS_FINDING]->(f:Finding {id:finding_idx})
+SET f += finding
+"""
+batched_import(community_statement, community_report_df)
+
+
+# 6、创建或更新community与entity、chunk节点之间的关系
+# 从指定的 Parquet 文件 create_final_communities.parquet 中读取列
+# 并将它们加载到一个名为 community_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+community_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_communities.parquet',
+                     columns=["id","level","title","text_unit_ids","relationship_ids"])
+# 打印输出数据帧 rel_df 的前 30 行内容
+print(community_df.head(30))
+# MERGE (c:__Community__ {community:value.id})查找或创建一个 __Community__ 节点，并将其 community 属性设置为 value.id 的值。如果具有该属性的节点已存在，则返回该节点；否则，创建新的节点
+# SET c += value {.level, .title}将 value 对象中的 level 和 title 属性赋值给 c 节点的同名属性
+# UNWIND value.text_unit_ids as text_unit_id将 value.text_unit_ids 列表中的每个元素展开为单独的记录，并命名为 text_unit_id
+# MATCH (t:__Chunk__ {id:text_unit_id})：查找具有对应 id 属性的 __Chunk__ 节点
+# MERGE (c)-[:HAS_CHUNK]->(t)：在 __Community__ 节点 c 和 __Chunk__ 节点 t 之间创建或查找 HAS_CHUNK 关系
+# WITH *：将当前查询上下文中的所有变量传递给接下来的查询部分。这里会保留 c 和 value 的上下文
+# UNWIND value.relationship_ids as rel_id将 value.relationship_ids 列表中的每个元素展开为单独的记录，并命名为 rel_id
+# MATCH (start:__Entity__)-[:RELATED {id:rel_id}]->(end:__Entity__)查找两个 __Entity__ 节点之间的 RELATED 关系，其中 id 属性等于 rel_id
+# MERGE (start)-[:IN_COMMUNITY]->(c)和MERGE (end)-[:IN_COMMUNITY]->(c)
+# MERGE在 start 节点和 c 节点之间创建或查找 IN_COMMUNITY 关系，以及在 end 节点和 c 节点之间创建或查找 IN_COMMUNITY 关系
+# 这样将两个 __Entity__ 节点与 __Community__ 节点关联起来
+# RETURN count(distinct c) as createdCommunities返回创建或更新的 __Community__ 节点的数量，并将结果命名为 createdCommunities
+statement = """
+MERGE (c:__Community__ {community:value.id})
+SET c += value {.level}
+WITH *
+UNWIND value.text_unit_ids as text_unit_id
+MATCH (t:__Chunk__ {id:text_unit_id})
+MERGE (c)-[:HAS_CHUNK]->(t)
+WITH *
+UNWIND value.relationship_ids as rel_id
+MATCH (start:__Entity__)-[:RELATED {id:rel_id}]->(end:__Entity__)
+MERGE (start)-[:IN_COMMUNITY]->(c)
+MERGE (end)-[:IN_COMMUNITY]->(c)
+RETURN count(distinct c) as createdCommunities
+"""
+batched_import(statement, community_df)
+
+
+# 7、处理与协变量 (__Covariate__) 相关的数据，并将这些协变量与特定的文本单元 (__Chunk__) 关联起来
+# 从指定的 Parquet 文件 create_final_covariates.parquet 中读取列
+# 并将它们加载到一个名为 cov_df 的 Pandas 数据帧中。这个数据帧可以进一步用于数据处理、分析或导入操作
+cov_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_covariates.parquet')
+# 打印输出数据帧 rel_df 的前 30 行内容
+print(cov_df.head(30))
+# MERGE (c:__Covariate__ {id:value.id})查找或创建一个具有标签 __Covariate__ 且属性 id 等于 value.id 的节点。如果该节点已经存在，则使用现有节点；如果不存在，则创建一个新的节点
+# SET c += apoc.map.clean(value, ["text_unit_id", "document_ids", "n_tokens"], [NULL, ""])
+# 使用 apoc.map.clean 函数来清理 value 字典，将不需要的键移除。具体来说，它会从 value 字典中移除 "text_unit_id", "document_ids", 和 "n_tokens" 这三个键，然后将剩余的键值对设置到节点 c 上
+# WITH c, value当前查询的结果（包括 c 和 value）传递给接下来的查询步骤。它相当于将结果暂存，允许在接下来的部分中继续使用这些结果
+# MATCH (ch:__Chunk__ {id: value.text_unit_id})查找具有标签 __Chunk__ 且 id 等于 value.text_unit_id 的节点。如果找到匹配的节点，就会将其存储在变量 ch 中
+# MERGE (ch)-[:HAS_COVARIATE]->(c)创建或查找一个从 ch 节点（__Chunk__）到 c 节点（__Covariate__）的 HAS_COVARIATE 关系。如果这个关系已经存在，Neo4j 会使用现有的关系；如果不存在，则创建新的关系
+cov_statement = """
+MERGE (c:__Covariate__ {id:value.id})
+SET c += apoc.map.clean(value, ["text_unit_id", "document_ids", "n_tokens"], [NULL, ""])
+WITH c, value
+MATCH (ch:__Chunk__ {id: value.text_unit_id})
+MERGE (ch)-[:HAS_COVARIATE]->(c)
+"""
+batched_import(cov_statement, cov_df)
+
+
+
+
+
+
+
+
+
+
Index: other/utils/graphrag3dknowledge.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/utils/graphrag3dknowledge.py b/other/utils/graphrag3dknowledge.py
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/utils/graphrag3dknowledge.py	(date 1734766590579)
@@ -0,0 +1,285 @@
+import os #用于文件系统操作
+import pandas as pd #用于数据处理和操作
+import networkx as nx #用于创建和分析图结构
+import plotly.graph_objects as go #plotly：用于创建交互式可视化 plotly.graph_objects：用于创建低级的plotly图形对象
+from plotly.subplots import make_subplots #用于创建子图
+import plotly.express as px #用于快速创建统计图表
+
+
+# 功能：读取指定目录下的所有Parquet文件并合并成一个DataFrame
+# 实现：使用os.listdir遍历目录，pd.read_parquet读取每个文件，然后用pd.concat合并
+def read_parquet_files(directory):
+    dataframes = []
+    for filename in os.listdir(directory):
+        if filename.endswith('.parquet'):
+            file_path = os.path.join(directory, filename)
+            df = pd.read_parquet(file_path)
+            dataframes.append(df)
+    return pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()
+
+
+# 功能：清理DataFrame，移除无效的行
+# 实现：删除source和target列中的空值，将这两列转换为字符串类型
+def clean_dataframe(df):
+    df = df.dropna(subset=['source', 'target'])
+    df['source'] = df['source'].astype(str)
+    df['target'] = df['target'].astype(str)
+    return df
+
+
+# 功能：从DataFrame创建知识图谱
+# 实现：使用networkx创建有向图，遍历DataFrame的每一行，添加边和属性
+def create_knowledge_graph(df):
+    G = nx.DiGraph()
+    for _, row in df.iterrows():
+        source = row['source']
+        target = row['target']
+        attributes = {k: v for k, v in row.items() if k not in ['source', 'target']}
+        G.add_edge(source, target, **attributes)
+    return G
+
+
+# 功能：创建节点和边的3D轨迹
+# 实现：使用networkx的布局信息创建Plotly的Scatter3d对象
+def create_node_link_trace(G, pos):
+    edge_x = []
+    edge_y = []
+    edge_z = []
+    for edge in G.edges():
+        x0, y0, z0 = pos[edge[0]]
+        x1, y1, z1 = pos[edge[1]]
+        edge_x.extend([x0, x1, None])
+        edge_y.extend([y0, y1, None])
+        edge_z.extend([z0, z1, None])
+
+    edge_trace = go.Scatter3d(
+        x=edge_x, y=edge_y, z=edge_z,
+        line=dict(width=0.5, color='#888'),
+        hoverinfo='none',
+        mode='lines')
+
+    node_x = [pos[node][0] for node in G.nodes()]
+    node_y = [pos[node][1] for node in G.nodes()]
+    node_z = [pos[node][2] for node in G.nodes()]
+
+    node_trace = go.Scatter3d(
+        x=node_x, y=node_y, z=node_z,
+        mode='markers',
+        hoverinfo='text',
+        marker=dict(
+            showscale=True,
+            colorscale='Viridis',
+            size=10,
+            colorbar=dict(
+                thickness=15,
+                title='Node Connections',
+                xanchor='left',
+                titleside='right'
+            )
+        )
+    )
+
+    node_adjacencies = []
+    node_text = []
+    for node, adjacencies in G.adjacency():
+        node_adjacencies.append(len(adjacencies))
+        node_text.append(f'Node: {node}<br># of connections: {len(adjacencies)}')
+
+    node_trace.marker.color = node_adjacencies
+    node_trace.text = node_text
+
+    return edge_trace, node_trace
+
+
+# 功能：创建边标签的3D轨迹
+# 实现：计算边的中点位置，创建Scatter3d对象显示标签
+def create_edge_label_trace(G, pos, edge_labels):
+    return go.Scatter3d(
+        x=[pos[edge[0]][0] + (pos[edge[1]][0] - pos[edge[0]][0]) / 2 for edge in edge_labels],
+        y=[pos[edge[0]][1] + (pos[edge[1]][1] - pos[edge[0]][1]) / 2 for edge in edge_labels],
+        z=[pos[edge[0]][2] + (pos[edge[1]][2] - pos[edge[0]][2]) / 2 for edge in edge_labels],
+        mode='text',
+        text=list(edge_labels.values()),
+        textposition='middle center',
+        hoverinfo='none'
+    )
+
+
+# 功能：创建节点度分布直方图
+# 实现：使用plotly.express创建直方图
+def create_degree_distribution(G):
+    degrees = [d for n, d in G.degree()]
+    fig = px.histogram(x=degrees, nbins=20, labels={'x': 'Degree', 'y': 'Count'})
+    fig.update_layout(
+        title_text='Node Degree Distribution',
+        margin=dict(l=0, r=0, t=30, b=0),
+        height=300
+    )
+    return fig
+
+
+# 功能：创建节点中心性分布箱线图
+# 实现：计算度中心性，使用plotly.express创建箱线图
+def create_centrality_plot(G):
+    centrality = nx.degree_centrality(G)
+    centrality_values = list(centrality.values())
+    fig = px.box(y=centrality_values, labels={'y': 'Centrality'})
+    fig.update_layout(
+        title_text='Degree Centrality Distribution',
+        margin=dict(l=0, r=0, t=30, b=0),
+        height=300
+    )
+    return fig
+
+
+# 功能：使用Plotly创建全面优化布局的高级交互式知识图谱可视化
+# 实现：
+#     创建3D布局
+#     生成节点和边的轨迹
+#     创建子图，包括3D图、度分布图和中心性分布图
+#     添加交互式按钮和滑块
+#     优化整体布局
+def visualize_graph_plotly(G):
+
+    if G.number_of_nodes() == 0:
+        print("Graph is empty. Nothing to visualize.")
+        return
+
+    pos = nx.spring_layout(G, dim=3)  # 3D layout
+    edge_trace, node_trace = create_node_link_trace(G, pos)
+
+    edge_labels = nx.get_edge_attributes(G, 'relation')
+    edge_label_trace = create_edge_label_trace(G, pos, edge_labels)
+
+    degree_dist_fig = create_degree_distribution(G)
+    centrality_fig = create_centrality_plot(G)
+
+    fig = make_subplots(
+        rows=2, cols=2,
+        column_widths=[0.7, 0.3],
+        row_heights=[0.7, 0.3],
+        specs=[
+            [{"type": "scene", "rowspan": 2}, {"type": "xy"}],
+            [None, {"type": "xy"}]
+        ],
+        subplot_titles=("3D Knowledge Graph by GraphRAG", "Node Degree Distribution", "Degree Centrality Distribution")
+    )
+
+    fig.add_trace(edge_trace, row=1, col=1)
+    fig.add_trace(node_trace, row=1, col=1)
+    fig.add_trace(edge_label_trace, row=1, col=1)
+
+    fig.add_trace(degree_dist_fig.data[0], row=1, col=2)
+    fig.add_trace(centrality_fig.data[0], row=2, col=2)
+
+    # Update 3D layout
+    fig.update_layout(
+        scene=dict(
+            xaxis=dict(showticklabels=False, showgrid=False, zeroline=False, showline=False, title='', backgroundcolor='rgb(255,255,255)'),
+            yaxis=dict(showticklabels=False, showgrid=False, zeroline=False, showline=False, title='', backgroundcolor='rgb(255,255,255)'),
+            zaxis=dict(showticklabels=False, showgrid=False, zeroline=False, showline=False, title='', backgroundcolor='rgb(255,255,255)'),
+            aspectmode='cube',
+            # bgcolor='rgb(0,0,0)'  # 设置背景颜色
+        ),
+        # paper_bgcolor='rgb(0,0,0)',  # 设置图表纸张背景颜色
+        # plot_bgcolor='rgb(0,0,0)',  # 设置绘图区域背景颜色
+        scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))
+    )
+
+    # Add buttons for different layouts
+    fig.update_layout(
+        updatemenus=[
+            dict(
+                type="buttons",
+                direction="left",
+                buttons=list([
+                    dict(args=[{"visible": [True, True, True, True, True]}], label="Show All", method="update"),
+                    dict(args=[{"visible": [True, True, False, True, True]}], label="Hide Edge Labels",
+                         method="update"),
+                    dict(args=[{"visible": [False, True, False, True, True]}], label="Nodes Only", method="update")
+                ]),
+                pad={"r": 10, "t": 10},
+                showactive=True,
+                x=0.05,
+                xanchor="left",
+                y=1.1,
+                yanchor="top"
+            ),
+        ]
+    )
+
+    # Add slider for node size
+    fig.update_layout(
+        sliders=[dict(
+            active=0,
+            currentvalue={"prefix": "Node Size: "},
+            pad={"t": 50},
+            steps=[dict(method='update',
+                        args=[{'marker.size': [i] * len(G.nodes)}],
+                        label=str(i)) for i in range(5, 21, 5)]
+        )]
+    )
+
+    # 优化整体布局
+    # fig.update_layout(
+    #     height=1198,  # 增加整体高度
+    #     width=2055,  # 增加整体宽度
+    #     title_text="Advanced Interactive Knowledge Graph",
+    #     margin=dict(l=10, r=10, t=25, b=10),
+    #     legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
+    # )
+
+    fig.show()
+
+
+# 功能：主函数，协调整个程序的执行流程
+#     实现：
+#         读取Parquet文件
+#         清理数据
+#         创建知识图谱
+#         打印图的统计信息
+#         调用可视化函数
+def main():
+
+    # 指定Parquet文件路径
+    directory = '/Users/janetjiang/Desktop/agi_code/GraphragTest/ragtest/inputs/artifacts'
+    # 读取指定目录下的所有Parquet文件并合并成一个DataFrame
+    df = read_parquet_files(directory)
+
+    if df.empty:
+        print("No data found in the specified directory.")
+        return
+
+    print("Original DataFrame shape:", df.shape)
+    print("Original DataFrame columns:", df.columns.tolist())
+    print("Original DataFrame head:")
+    print(df.head())
+    # 清理DataFrame，移除无效的行
+    df = clean_dataframe(df)
+
+    print("\nCleaned DataFrame shape:", df.shape)
+    print("Cleaned DataFrame head:")
+    print(df.head())
+
+    if df.empty:
+        print("No valid data remaining after cleaning.")
+        return
+
+    # 从DataFrame创建知识图谱
+    G = create_knowledge_graph(df)
+
+    print(f"\nGraph statistics:")
+    print(f"Nodes: {G.number_of_nodes()}")
+    print(f"Edges: {G.number_of_edges()}")
+
+    if G.number_of_nodes() > 0:
+        # 将图G转换为无向图。如果G是有向图，转换为无向图后才能正确计算连通分量
+        print(f"Connected components: {nx.number_connected_components(G.to_undirected())}")
+        # 对图G进行可视化
+        visualize_graph_plotly(G)
+    else:
+        print("Graph is empty. Cannot visualize.")
+
+
+if __name__ == "__main__":
+    main()
Index: other/temp/requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/temp/requirements.txt b/other/temp/requirements.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/temp/requirements.txt	(date 1734766590579)
@@ -0,0 +1,19 @@
+fastapi==0.112.0
+uvicorn==0.30.6
+pandas==2.2.2
+tiktoken==0.7.0
+graphrag==0.3.0
+pydantic==2.8.2
+python-dotenv==1.0.1
+asyncio==3.4.3
+aiohttp==3.10.3
+numpy==1.26.4
+scikit-learn==1.5.1
+matplotlib==3.9.2
+seaborn==0.13.2
+nltk==3.8.1
+spacy==3.7.5
+transformers==4.44.0
+torch==2.2.2
+torchvision==0.17.2
+torchaudio==2.2.2
Index: .idea/inspectionProfiles/Project_Default.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/Project_Default.xml b/.idea/inspectionProfiles/Project_Default.xml
new file mode 100644
--- /dev/null	(date 1734766589031)
+++ b/.idea/inspectionProfiles/Project_Default.xml	(date 1734766589031)
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <profile version="1.0">
+    <option name="myName" value="Project Default" />
+    <inspection_tool class="PyUnresolvedReferencesInspection" enabled="false" level="INFORMATION" enabled_by_default="false" />
+  </profile>
+</component>
\ No newline at end of file
Index: .idea/inspectionProfiles/profiles_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
--- /dev/null	(date 1734766589031)
+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1734766589031)
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
Index: other/temp/settings.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/temp/settings.yaml b/other/temp/settings.yaml
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/temp/settings.yaml	(date 1734766590579)
@@ -0,0 +1,165 @@
+encoding_model: cl100k_base
+skip_workflows: []
+llm:
+  api_key: ${GRAPHRAG_CHAT_API_KEY}
+  type: openai_chat # or azure_openai_chat
+  model: ${GRAPHRAG_CHAT_MODEL}
+  model_supports_json: true # recommended if this is available for your model.
+  max_tokens: 2000
+  # request_timeout: 180.0
+  # api_base: https://<instance>.openai.azure.com
+  api_base: ${GRAPHRAG_API_BASE}
+  # api_version: 2024-02-15-preview
+  # organization: <organization_id>
+  # deployment_name: <azure_model_deployment_name>
+  # tokens_per_minute: 150_000 # set a leaky bucket throttle
+  # requests_per_minute: 10_000 # set a leaky bucket throttle
+  # max_retries: 10
+  # max_retry_wait: 10.0
+  # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times
+  # concurrent_requests: 25 # the number of parallel inflight requests that may be made
+  # temperature: 0 # temperature for sampling
+  # top_p: 1 # top-p sampling
+  # n: 1 # Number of completions to generate
+
+parallelization:
+  stagger: 0.3
+  # num_threads: 50 # the number of threads to use for parallel processing
+
+async_mode: threaded # or asyncio
+
+embeddings:
+  ## parallelization: override the global parallelization settings for embeddings
+  async_mode: threaded # or asyncio
+  llm:
+    api_key: ${GRAPHRAG_EMBEDDING_API_KEY}
+    type: openai_embedding # or azure_openai_embedding
+    model: ${GRAPHRAG_EMBEDDING_MODEL}
+    # api_base: https://<instance>.openai.azure.com
+    api_base: ${GRAPHRAG_API_BASE}
+    # api_version: 2024-02-15-preview
+    # organization: <organization_id>
+    # deployment_name: <azure_model_deployment_name>
+    # tokens_per_minute: 150_000 # set a leaky bucket throttle
+    # requests_per_minute: 10_000 # set a leaky bucket throttle
+    # max_retries: 10
+    # max_retry_wait: 10.0
+    # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times
+    # concurrent_requests: 25 # the number of parallel inflight requests that may be made
+    # batch_size: 16 # the number of documents to send in a single request
+    # batch_max_tokens: 8191 # the maximum number of tokens to send in a single request
+    # target: required # or optional
+
+
+
+chunks:
+  size: 1200
+  overlap: 100
+  group_by_columns: [id] # by default, we don't allow chunks to cross documents
+
+input:
+  type: file # or blob
+  file_type: text # or csv
+  # base_dir: "input"
+  base_dir: ${GRAPHRAG_INPUT_DIR}
+  file_encoding: utf-8
+  file_pattern: ".*\\.txt$"
+
+cache:
+  type: file # or blob
+  # base_dir: "cache"
+  base_dir: ${GRAPHRAG_CACHE_DIR}
+  # connection_string: <azure_blob_storage_connection_string>
+  # container_name: <azure_blob_storage_container_name>
+
+storage:
+  type: file # or blob
+  # base_dir: "output/${timestamp}/artifacts"
+  # base_dir: "inputs/artifacts"
+  base_dir: ${GRAPHRAG_STORAGE_DIR}
+  # connection_string: <azure_blob_storage_connection_string>
+  # container_name: <azure_blob_storage_container_name>
+
+reporting:
+  type: file # or console, blob
+  # base_dir: "inputs/reports"
+  base_dir: ${GRAPHRAG_REPORTING_DIR}
+  # connection_string: <azure_blob_storage_connection_string>
+  # container_name: <azure_blob_storage_container_name>
+
+entity_extraction:
+  ## llm: override the global llm settings for this task
+  ## parallelization: override the global parallelization settings for this task
+  ## async_mode: override the global async_mode settings for this task
+  # prompt: "prompts/entity_extraction.txt"
+  prompt: ${GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE}
+  entity_types: [organization,person,geo,event]
+  max_gleanings: 1
+
+summarize_descriptions:
+  ## llm: override the global llm settings for this task
+  ## parallelization: override the global parallelization settings for this task
+  ## async_mode: override the global async_mode settings for this task
+  # prompt: "prompts/summarize_descriptions.txt"
+  prompt: ${GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE}
+  max_length: 500
+
+claim_extraction:
+  ## llm: override the global llm settings for this task
+  ## parallelization: override the global parallelization settings for this task
+  ## async_mode: override the global async_mode settings for this task
+  # 开启协变量
+  enabled: true
+  # prompt: "prompts/claim_extraction.txt"
+  prompt: ${GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE}
+  description: "Any claims or facts that could be relevant to information discovery."
+  max_gleanings: 1
+
+community_reports:
+  ## llm: override the global llm settings for this task
+  ## parallelization: override the global parallelization settings for this task
+  ## async_mode: override the global async_mode settings for this task
+  # prompt: "prompts/community_report.txt"
+  prompt: ${GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE}
+  max_length: 2000
+  max_input_length: 8000
+
+cluster_graph:
+  max_cluster_size: 10
+
+embed_graph:
+  enabled: false # if true, will generate node2vec embeddings for nodes
+  # num_walks: 10
+  # walk_length: 40
+  # window_size: 2
+  # iterations: 3
+  # random_seed: 597832
+
+umap:
+  enabled: false # if true, will generate UMAP embeddings for nodes
+
+snapshots:
+  graphml: false
+  raw_entities: false
+  top_level_nodes: false
+
+local_search:
+  # text_unit_prop: 0.5
+  # community_prop: 0.1
+  # conversation_history_max_turns: 5
+  # top_k_mapped_entities: 10
+  # top_k_relationships: 10
+  # llm_temperature: 0 # temperature for sampling
+  # llm_top_p: 1 # top-p sampling
+  # llm_n: 1 # Number of completions to generate
+  # max_tokens: 12000
+
+global_search:
+  # llm_temperature: 0 # temperature for sampling
+  # llm_top_p: 1 # top-p sampling
+  # llm_n: 1 # Number of completions to generate
+  # max_tokens: 12000
+  # data_max_tokens: 12000
+  # map_max_tokens: 1000
+  # reduce_max_tokens: 2000
+  # concurrency: 32
Index: other/text/3.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/3.txt b/other/text/3.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/3.txt	(date 1734766590579)
@@ -0,0 +1,39 @@
+第3回 齐天大圣大闹天宫
+
+  　　冥司阎王和龙王先后都来找玉皇大帝，状告孙悟空大闹龙宫和地府。玉皇大帝正要派天兵、天将到人间去收伏孙悟空。这时，太白金星走了出来，给玉帝出了个主意，说不如随便给他一个官职，把他困在天上，玉帝同意了，命文曲星写了一封诏书，叫太白金星请悟空上天。 
+　　太白金星遵命来到花果山，宣读圣旨。孙悟空听了十分高兴，就命令猴子们看家，自己跟着太白金星驾着云来到灵霄殿上。太白金星向玉帝行了礼，说∶“悟空来了。”玉帝问∶“谁是悟空？”悟空听了，既不行礼，也不跪拜，随便答应了一声∶“我就是。”其他神仙见悟空没有礼貌都非常生气。 
+　　玉帝对悟空没有办法，听了武曲星君的建议让悟空给玉帝看马。这个官职在天上是最小的，过了半个月悟空才知道。一气之下，便拿出金箍棒，杀出南天门，回到花果山，自封“齐天大圣”。又做了一面大旗，插在花果山上。 
+　　玉帝听说孙悟空又回到花果山，马上命令托塔李天王和三太子哪吒，带兵去捉拿悟空。没想到先锋官巨灵神和悟空没打几个回合，宣花斧就成了两截。哪吒一见气得头发都竖了起来，大喊一声，变成三头六臂，拿着六件兵器和悟空打了起来。 
+　　悟空也不示弱，摇身一变，也变成三头六臂，拿着三根金箍棒跟哪吒打了好长时间，仍不分胜负。悟空偷偷拔下一根毫毛变成自己，跟哪吒打，真身却绕到哪吒身后，举起棒子就打。哪吒躲闪不及，被打中左臂，痛得也顾不上还手，转身就跑。 
+　　玉帝听了这些十分生气，准备多派些兵将，再去和孙悟空打。这时太白金星又出了个主意说∶“不如封孙悟空一个有名无权的齐天大圣，什么事也不让他管，只把他留在天上，免得再派人去打，伤了兵将。”玉帝听了觉得有理，于是派太白金星去讲和。 
+　　悟空听说后，十分高兴，跟太白金星又一次来到天宫。玉帝马上让人在蟠桃园右侧为孙悟空修了一座齐天大圣府。孙悟空到底是个猴子，只知道名声好听，也不问有没有实权，整天和天神们以兄弟相称，在府内吃喝玩乐，今天东游游，明天西转转，自由自在。 
+　　时间长了，玉帝怕悟空闲着没事添麻烦，就让他去管蟠桃园。这桃园前、中、后各有桃树一千二百棵。前面的树三千年结果成熟，吃了可以成仙；中间的树六千年结果成熟，吃了能长生不老；后面的树九千年结果成熟，吃了以后可以跟日月同辉，天地齐寿。 
+　　一天，他见园中的桃子大部分都熟了，就想尝个新鲜，便偷偷地跑进园子去，脱了衣服，爬上大树，挑熟透的大桃吃了个饱。从此以后，每隔两三天，他就设法偷吃一次桃。每年一次的蟠桃会到了，一天，七位仙女奉王母娘娘之命进园摘桃。 
+　　恰巧这时孙悟空把桃吃饱了，感到有点困，就变成二寸来长的小人，在大树梢上，找个凉快的地方睡着了。七位仙女见园中的熟桃不多，便四处寻找，找了好长的一段时间，最后在一棵大树梢上发现有个熟透的桃，就把树梢扯下来。 
+　　没想到悟空正好睡在这棵树上，被惊醒了，变回原来的样子。他拿出金箍棒叫了声∶“谁敢偷桃？”吓得七位仙女一齐跪下，说明了来这的原因。 
+　　悟空问蟠桃会请了什么人，当他知道没有自己时，十分生气。 
+　　他用定身法把七位仙女定住，然后驾着云来到瑶池。这时赶来赴宴的众仙还没有到，只有佣人在摆设宴席，于是悟空拔了根毫毛，变成瞌睡虫，放到佣人脸上。这些人立刻呼呼大睡，他跳到桌上，端起美酒，开怀痛饮。 
+　　他吃饱喝足后才走出瑶池，迷迷糊糊地走到太上老君的兜率宫里，刚好宫里没有人就把五个葫芦里的金丹全部倒出来吃了，吃完这才想到闯了大祸，可能保不住性命。于是又回到瑶池，偷了几罐好酒，回花果山去了。 
+　　玉帝听到报告，大发脾气，命令李天王和哪吒太子率领十万天兵天将，布下十八层天罗地网，一定要捉拿悟空回来。但是天兵天将都不是悟空的对手，一个个都败下来。于是观音菩萨就建议让灌江口的显圣二郎神到花果山来捉拿孙悟空。 
+　　二郎神奉命，带领梅山六兄弟，点了些精兵良将，杀向花果山。他请李天王举着照妖镜站在空中，对着悟空照，自己到水帘洞前挑战。悟空出洞迎战，与二郎神打得难分难解。梅山六兄弟见悟空这时顾不上他们，就乘机杀进了水帘洞。 
+　　悟空见自己的老窝被破坏了，心里一慌，变成麻雀想跑，二郎神摇身变成了捉麻雀的鹰，抖抖翅膀就去啄麻雀；悟空急忙又变成一只大鹚鸟，冲向天空，二郎神急忙变成了一只大海鹤，钻进云里去扑；悟空一见嗖地一声飞到水里，变成一条鱼。 
+　　二郎神从照妖镜里看见了悟空，就变成鱼鹰，在水面上等着，悟空见了，急忙变条水蛇，窜到岸边，接着又变成花鸨，立在芦苇上。二郎神见他变的太低贱，也不去理他，变回原来的样子，取出弹弓，朝着花鸨就打，把悟空打得站立不稳。 
+　　悟空趁机滚下山坡，变成一座土地庙，二郎神追过来，见有个旗杆立在庙的后面，就知道是悟空变的，拿起兵器就朝门砸过去，悟空见被看出来了，往上一跳，变回原样就跑，二郎神驾着云追了过去。两个人一边走一边打，又来到花果山跟前。 
+　　各路的天兵神将一拥而上，把悟空团团围住，在南天门观战的太上老君趁机把金钢琢朝悟空扔过去，悟空被打中头部，摔了一跤。二郎神的哮天犬跑上去，咬住了悟空，其他天神则扑上去把悟空按住，用铁链穿住琵琶骨捆了回去。 
+　　孙悟空被绑在斩妖台上，但不论用刀砍斧剁，还是用雷打火烧，都不能伤他一根毫毛。太上老君启奏玉帝，把悟空放到八卦炉里熔炼，玉帝准奏。 
+　　于是，悟空被带到兜率宫，众神仙把他推进八卦炉里，烧火的童子用扇子使劲扇火。 
+　　悟空在炉中跳来跳去，偶然中跳到巽宫的位置，这里只有烟没有火，熏得很厉害，就弯下身子蹲在里面。四十九天过去了，太上老君下令打开炉门，悟空忽然听到炉顶有响声，抬头看见一道光，用力一跳，跳出炼丹炉，踢倒炉子，转身就跑。 
+　　孙悟空不但没有被熔化，反而炼就了一双火眼金睛。他从耳朵中掏出金箍棒，迎风一晃，变成碗口那么粗。悟空抡起如意棒，一路指东打西，直打到灵霄殿上，大声叫喊着∶“皇帝轮流做，玉帝老头，你快搬出去，把天宫让给我，要不，就给你点厉害看看！” 
+　　幸好有三十六员雷将，二十八座星宿赶来保护，玉帝才能脱身。玉帝立即派人去西天请如来佛祖。如来一听，带着阿傩、伽叶两位尊者，来到灵霄殿外，命令停止打斗，叫悟空出来，看看他有什么本事。悟空怒气冲冲地看着如来，根本就不把如来放在眼里。 
+　　如来佛祖伸开手掌说∶“如果你有本领一筋斗翻出我的手掌，我就劝玉帝到西方去，把位子让给你。”悟空一听不知道是计，心里还挺高兴，就把金箍棒放在耳朵里，轻轻一跳，站在如来佛的手心中，喊到∶“我去了！” 
+　　一个筋斗，无影无踪。 
+　　悟空驾着云飞一样地往前赶，忽然见前面有五根肉红色的柱子，想这肯定是天边了，柱子一定是撑天用的，这才停下来。他害怕回去见如来没有凭证，就拔下一根毫毛，变成一支笔，在中间的一根柱子上写下“齐天大圣到此一游”八个大字。 
+　　写完收了毫毛，又跑到第一个柱子下撒了一泡猴尿，然后又驾起筋斗云，回到如来佛祖手掌里说∶“如果你说话算数，就快叫玉帝让位子吧！”如来佛却说孙悟空根本没有离开他的掌心。悟空不服，要如来去看看他在天边留下的证据。 
+　　如来佛不去，他让悟空看看他右手的中指，再闻闻大拇指根。悟空睁大火眼金睛，只见佛祖右手中指上有他写的那八个大字，大拇指丫里还有些猴尿的臊气。悟空吃惊地说∶“我不信，我一点也不信，我把字写在撑天的柱子上，怎么却在你手上？等我去看看再说。” 
+　　悟空转身想跑，如来佛眼疾手快，反手一扑，把悟空推出西天门外，又把手的五指分别化作金、木、水、火、土五座联山，给这座联山起名叫“五行山”，将悟空牢牢压在山下。天上的各位神仙和阿傩、伽叶一个个合掌称好。 
+　　玉帝见如来佛祖镇压了孙悟空，心里十分高兴，立即传令设下“安天大会”感谢佛祖。不一会，各路神仙都被请来了，玉帝又命令打开太玄宫、洞阳玉馆，请如来佛坐在七宝灵台上，各路神仙纷纷送来贺礼，如来佛命阿傩、伽叶将礼物一一收下。 
+　　就在众位神仙饮酒听歌的时候，巡查官回来报告∶“那个妖猴把头伸出来了！”佛祖一听，就从袖子里取出一张帖子，上面写着∶“、嘛、呢、叭、、”，叫阿傩、伽叶拿去贴在五行山顶的一块方石头上，那座山的缝立刻合住，孙悟空再也没有办法出来了。 
+　　如来佛祖回西天时，路过五行山，又发了慈悲心，叫来山神，让他和五方揭谛住在这座山上，监押悟空，并对他们说∶“如果他饿了，就给他吃些铁丸子，渴了，就把溶化的铜水给他喝。五百年以后，他刑期满了，自然会有人来救他。”
+
+  
+		
\ No newline at end of file
Index: other/text/8.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/8.txt b/other/text/8.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/8.txt	(date 1734766590579)
@@ -0,0 +1,31 @@
+第8回 黄风洞唐三藏有难
+
+  　　从此以后，唐僧又多了一个徒弟。师徒三人不怕千辛万苦，日夜赶路向西前进。这天来到一座很险峻的山下，忽然刮来一阵旋风。悟空让过了风头，一把抓住风尾闻了闻，有一股腥臭气，说∶“闻这风的味儿，说明附近不是有猛虎就是有妖怪。” 
+　　话还没说完，山坡下就跳出一只斑斓猛虎，把唐僧吓得从白马上滚了下来。八戒一见，扔下行李，拿起钉耙劈头就打。忽然那只猛虎站了起来，伸出爪子往前胸一抓，把虎皮剥下来，高声叫道∶“我是黄风大王的先锋，你们是哪里的和尚？” 
+　　八戒刚说明来历，那妖精一转身从乱石中取出两口赤铜刀，朝着八戒就砍，两个在坡前打了起来。悟空一见，拿着金箍棒上前就打。那妖精见不是对手，就地一滚，又变成猛虎跑了。悟空和八戒哪里肯罢休，连忙紧追而去。 
+　　那妖精见脱不了身，就使了个金蝉脱壳之计，用爪子剥下虎皮，盖在一块卧虎石上，自己变成一股狂风给溜了。到了路口，见唐僧一个人端坐在路口，就一把抓起唐僧，驾着狂风跑了。 
+　　那妖精把唐僧抓到洞中，洋洋得意地给洞主说他抓了唐僧。老妖怪一听，大吃一惊。原来他知道孙悟空的厉害，心里十分害怕，就叫小妖们把唐僧拉到后园子里，绑在定风柱上，先不要吃，等过上三、五天，如果唐僧的徒弟不来捣乱的话，那时再吃也不迟。 
+　　再说悟空和八戒见那只老虎趴在块石头上，棒耙一齐打了下去，虎皮碎了，石头裂了，悟空大叫道∶“不好，中计了！”两人急忙回到路口去找师父，哪里还有师父的影子，八戒急得大哭，悟空说∶“先不要哭，我们应该先找回师父！” 
+　　师兄弟两个追进山中，穿山越岭，忽然看见一块岩石下有座山门，上书六个大字“黄风岭黄风洞”。悟空让八戒看着马和行李，自己到洞口叫阵。 
+　　小妖们忙跑进去向老妖报告。 
+　　老妖怪的先锋自告奋勇，点齐五十名精壮部下，打着战鼓摇着战旗，提着两口赤铜刀，跳出洞门，与悟空打了起来。几个回合下来，那妖怪就腰酸腿痛，转身想跑，悟空连忙拦住去路，先锋一见回不成山洞了，只好转身向山坡上跑。 
+　　八戒正好在山坡上放马，见妖精被悟空追得往这边跑，就放开马，举起钉耙用力打去，一下把妖怪的脑壳打得稀巴烂，现出了原形——是一只老虎。悟空见了很高兴，一手提金箍棒，一手拖着死老虎，又来到洞口，要引那老妖出来。 
+　　老妖听说悟空拖着先锋官的尸体前来骂阵，气得不得了，立即穿戴整齐，拿一杆三股钢叉，率领所有的小妖出洞应战。悟空见了老妖便直扑过去，两个人在黄风洞口，你一叉，我一棒，打得难解难分。 
+　　悟空求胜心切，便从身上拔下一把毫毛，吹了口仙气，这些毫毛立刻变成了一百多个悟空，每人手拿一根金箍棒，把老妖团团围住。老妖哪里遇见过这场面，心中十分惧怕，朝地上吹了口气，顿时天上刮起一阵狂风，把那一百多个悟空吹得像风车一样在空中乱转。 
+　　悟空急忙把毫毛收回身上，自己举着金箍棒去跟老妖斗。老妖知道难胜悟空，于是猛地朝悟空脸上吹了口气，悟空顿时觉得眼睛似乎被无数个针尖猛扎，疼得睁不开眼睛，立刻败下阵来。老妖乘机收兵回去了。这时天也晴了，悟空对八戒说双眼酸痛，泪流不止。 
+　　师兄弟两个见天也晚了，决定找个地方歇歇脚，明日再找老妖算帐。于是他们牵着马走出山坳，见南山坡下有一院子，就走进去敲门，请求借宿，没想到一个老大爷带着几个年轻农夫，手中拿着棍棒冲了出来。 
+　　悟空想是误会了，急忙说明来意。那老大爷听后连忙赔礼，又叫人收拾床铺准备饭菜。悟空眼睛泪流不止，老人家拿出一个玛瑙石做的小罐子，说是有个仙人传给他的一种药叫三花九子膏，专治一切风眼。老大爷给悟空眼睛点了一些，让他不要睁开。 
+　　悟空干脆躺下就睡，直睡到第二天早晨才醒，他使劲眨了眨眼，不痛了。更奇怪的是，师兄弟俩竟躺在草地上。往四周望一望，见树上有张条子，取下来一看，才知道昨天晚上的院子，老大爷都是护法伽蓝变的。两人商量后决定，仍由八戒看马，悟空前去捉妖。 
+　　悟空来到黄风洞前，变成一只花脚蚊虫，从门缝飞了进去，见老妖正在大厅和小妖们收拾兵器，准备再次交战。悟空飞到后院，见师父被绑在定风柱上，就飞到唐僧头上，轻声地安慰了一番，让他不用担心害怕。 
+　　悟空又飞回大厅，正好碰见一个小妖来报∶“大王，只有一个长嘴大耳朵的和尚坐在树林里，那个毛脸和尚不见了，说不定是搬兵去了。”老妖说∶“怕什么，除了灵吉菩萨什么人我都不怕！”悟空听了，心中一阵欢喜。 
+　　他飞出黄风洞，现出原形后来到林中，把刚才听到的都给八戒说了一遍。可是到哪儿去找灵吉菩萨呢？两人正在商量，忽见大路旁走出一位老公公，悟空连忙施礼，问道∶“请问老人家，您可知道灵吉菩萨的住处？” 
+　　老公公说∶“灵吉菩萨住在正南方的小须弥山中。”，说着手向南指，悟空顺着手指向南望去，那老公公早已化成一股清风不见了，留下一张条子，原来是太白金星暗中相助。悟空让八戒藏在树林深处等他，自己则去找灵吉菩萨。 
+　　悟空一个筋斗直往南行，不一会儿就看见一座彩云围绕的高山，山谷里有座幽静的禅院，不时传出庄严的钟声，空气中弥漫着香气，就按住云头，走到门前，看见一个道士，急忙上前施礼，打听到这里就是灵吉菩萨讲经的地方。 
+　　灵吉菩萨听到通报后，立即出来迎接悟空，悟空说明了来意，菩萨说∶“我奉如来佛祖的法旨，在这里镇压黄风怪，原来我抓住他，但没有杀他，放到山中不许他再伤害生灵，没想到他仍恶性不改，竟然伤害你师父，我一定要助你一臂之力。” 
+　　灵吉菩萨拿出定风丹，又取了飞龙宝杖，与悟空驾云来到黄风山上。他让悟空到山门前去挑战，引诱黄风怪出来。悟空下了祥云，挥舞着金箍棒打破了洞门，那老妖非常生气，举叉便向悟空胸口刺来。 
+　　悟空举棒招架着，没打多久，那老妖张嘴又想呼风，半空中的灵吉菩萨扔下飞龙宝杖，变成一条八爪金龙，伸出两个爪子，一把抓住那老妖，提着头摔在石崖边上，妖怪现出原形，原来是一只黄毛貂鼠。 
+　　悟空举棒就想打，灵吉菩萨拦住说∶“慢着，他本是灵山脚下的老鼠，因为偷吃了琉璃灯里的清油，怕金刚捉他才跑到这里成精作怪。他既现了原形，就让我把他抓去见如来，看陷害唐僧该怎样处置他。悟空，你看这样可好？” 
+　　悟空谢了灵吉菩萨，灵吉菩萨带着黄毛貂鼠向西去了。悟空在林中找到八戒，两个把洞中的大小妖怪全都打死，救出唐僧，又在洞里找到些素食，做些饭侍候师父吃了，这才走到洞外，一把火烧了黄风洞，又继续上路西行。
+
+  
+		
\ No newline at end of file
Index: other/text/9.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/9.txt b/other/text/9.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/9.txt	(date 1734766590579)
@@ -0,0 +1,18 @@
+第9回 流沙河沙和尚拜师
+
+  　　唐僧师徒三人过了黄风岭，一路上特别小心。天亮赶路，天黑就休息，这样过了一年。这天来到了一条一望无际，汹涌澎拜的大河边。悟空跳到空中一看，估计这条河少说了有八百里宽，不但看不到渡船，连个人影都没有。 
+　　突然八戒叫道∶“师兄，快到这儿来！”原来岸边有一块石碑，走近一看，碑上刻着“流沙河”三个大字，碑背面四行小字“八百流沙界，三千弱水深，鹅毛飘不起，芦花定底沉”。唐僧倒吸一口冷气说∶“这可怎么办？ 
+　　”突然，一声巨响，河中钻出一个妖怪来。 
+　　那妖怪朝唐僧扑了过来。悟空慌忙护着师父，八戒挥着钉耙，与妖怪在河边打了起来，半天仍不分胜负。悟空纵身一跃，举起棒子朝妖怪打去。那妖怪挥杖一挡，震得双臂发麻，虎口迸裂。 
+　　那妖怪慌忙跳到河里，钻入水底。悟空要去追赶，被唐僧叫住，师徒二人商量着如何过河。悟空说∶“这妖孽一定是住在这河中，想必有过河的办法，只要降服了他，就能过河！”八戒连忙说∶“俺老猪原是天河里的天蓬元帅，还是让我这懂水性的去收拾他吧！” 
+　　八戒念了个“避水咒”，脱了衣服拿着耙子钻进水府。妖怪举杖便打。 
+　　两个从水底打到水面，又从水面打到水底，整整两个时辰仍不分胜负。悟空插不上手，急得在旁边挤眉弄眼地作手势，要八戒把妖怪引到岸上来。 
+　　八戒正打在兴头上，哪能看懂悟空的意思。悟空急了，忍不住一个筋斗跳到云头，变成一只饿鹰扑落下来。那妖怪忽然听到头上有风声，抬头见悟空对着自己冲下来，就收起宝杖，一下扎进水里，再也不出来。 
+　　悟空没有办法，只好回来，对师父说∶“那妖怪非常狡猾，钻到水里后怎么也不出来。现在我去找观音菩萨想想办法。”悟空一个筋斗来到南海落伽山紫竹林中，找到观音说明情况，观音说∶“那妖怪原是天上的卷帘大将下凡，被我劝化，答应保唐僧西天取经的。” 
+　　观音菩萨派木叉行者带着一个红葫芦，和悟空来到流沙河。木叉行者驾云来到河面上，高声喊到∶“悟净！悟净！取经的人就在这儿，快点出来跟师父去吧！” 
+　　那妖怪听到呼唤，连忙钻出水面，木叉行者说∶“前来见你的师父和师兄们！”那妖怪整整衣服，拜见了师父，唐僧很高兴又收了个徒弟，给他剃了头，取名沙和尚。 
+　　沙和尚取下脖子上挂的九个骷髅，用绳子一串，又把观音菩萨的红葫芦拴在当中，放到河里，立刻变成一只小船。唐僧在八戒和悟净的搀扶下上了船，向西岸驶去。悟空牵着白龙马，在船后紧紧跟随。 
+　　到了岸上，木叉行者收起了红葫芦，那些骷髅立刻化成九股阴风一会儿就不见了。唐僧拜谢了木叉行者，又向南边谢了观音菩萨，然后才跨上白马，带着三个徒弟又向西赶路。
+
+  
+		
\ No newline at end of file
Index: .idea/modules.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/modules.xml b/.idea/modules.xml
new file mode 100644
--- /dev/null	(date 1734766589035)
+++ b/.idea/modules.xml	(date 1734766589035)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/GraphRAG4OpenWebUI-main.iml" filepath="$PROJECT_DIR$/.idea/GraphRAG4OpenWebUI-main.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
Index: other/text/2.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/2.txt b/other/text/2.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/2.txt	(date 1734766590579)
@@ -0,0 +1,22 @@
+第2回 闹龙宫刁石猴借宝
+
+  　　孙悟空见没办法留下来，就拜别了菩提祖师，又和各位师兄告别，然后念了口诀，驾着筋斗云，不到一个时辰，就回到了花果山水帘洞，看到花果山上一片荒凉破败的景象，很是凄惨。 
+　　原来孙悟空走了以后，有一个混世魔王独占了水帘洞，并且抢走了许多猴子猴孙。孙悟空听到这些以后，气得咬牙跺脚。他问清了混世魔王的住处，决定找混世魔王报仇，便驾着筋斗云，朝北方飞去。 
+　　不一会儿，孙悟空就来到混世魔王的水脏洞前，对门前的小妖喊到∶“你家那个狗屁魔王，多次欺负我们猴子。我今天来，要和那魔王比比高低！ 
+　　”小妖跑进洞里，报告魔王。魔王急忙穿上铁甲，提着大刀，在小妖们的簇拥下走出洞门。 
+　　孙悟空赤手空拳，夺过了混世魔王的大刀，把他劈成了两半。然后，拔下一把毫毛咬碎喷了出去，毫毛变成许多小猴子，直杀进洞里，把所有的妖精全杀死，然后救出被抢走的小猴子，放了一把火烧了水脏洞。 
+　　孙悟空收回了毫毛，让小猴子们闭上眼睛，作起法术来，一阵狂风刮过，他们驾着狂风回到了花果山。从此，孙悟空便叫小猴们做了些竹枪和木刀，用夺来的大刀教他们武艺。没过多久，孙悟空觉得竹枪木刀不能打仗，两个猴告诉他，傲来国里肯定有好的兵器。 
+　　孙悟空驾云来到傲来国上空，念起咒语，立即天空刮起狂风，砂石乱飞，把满城的军民吓得不敢出来。他趁机跑进兵器库拔了把毫毛一吹，变成上千个小猴，乱搬乱抢，悟空见差不多了，把风向一变回了花果山。 
+　　从此以后，花果山水帘洞的名气就更大了，所有的妖怪头子，即七十二洞的洞主都来拜见孙悟空。可是，悟空却有一件事不顺心，嫌那口大刀太轻，不好用。有个通背老猿猴告诉悟空，水帘洞桥下，可直通东海龙宫，叫他去找龙王要一件得心应手的兵器。 
+　　悟空立刻来到东海龙宫，给老龙王敖广讲明了来这儿的目的。龙王不好推辞，叫虾兵们抬出一杆三千六百斤重的九股叉，悟空接过来玩了一阵，嫌它太轻。龙王又命令蟹将们抬出一柄七千二百斤重的方天画戟，悟空一见，仍然嫌它太轻。 
+　　龙王说∶“再也没有比这更重的兵器了。”悟空不信，和龙王吵了起来，龙婆给龙王说∶“大禹治水时，测定海水深浅的神珍铁最近总是放光，就把这给他，管他能不能用，打发他走算了。”龙王听后告诉悟空∶“这宝物太重了，你自己去取吧！” 
+　　孙悟空跟龙王来到海底，龙王用手一指说∶“放光的就是。”悟空见神珍铁金光四射，就走过去用手一摸，原来是根铁柱子，斗一样粗，二丈多长。孙悟空使劲用手搬了搬说∶“太长太长了，要是再短些，再细一些就好了。” 
+　　孙悟空话还没有说完，那个宝贝就短了几尺，也细了一圈。孙悟空看了看说∶“再细些就更好了。”那个宝贝真的又细了许多，悟空拿过来，见上面写着∶“如意金箍棒、重一万三千五百斤”顺手玩了一会儿，觉得十分好用。 
+　　回到水晶宫，孙悟空又要龙王送一身衣服相配。龙王实在没有，但又害怕悟空乱打乱闹，只好敲响应急的金钟，叫来南、北、西三海龙王敖钦、敖顺和敖闰，兄弟三人凑了一副黄金甲、一顶凤翅紫金冠、一双藕丝步云鞋，送给悟空。 
+　　回到花果山，悟空才发现那根金箍棒竟可以变成绣花针一样大小，藏到耳朵中。一天，他宴请所有的妖王吃饭，喝醉了，在桥边的松树下睡觉，迷迷糊糊地见两个人手里拿着写有“孙悟空”的批文，走到他身边也不说话，把他用绳索套上，拉起来就走。 
+　　悟空糊里糊涂跟他们来到一座城门外，看见城楼上有一块牌子，牌子上写着“幽冥界”三个大字，知道这里是阎王住的地方，转身就要走，两个勾魂鬼死死拉住他，非要让他进去。孙悟空一看火了，从耳朵中掏出了金箍棒，把两个勾魂鬼打成了肉酱。 
+　　他甩掉套在身上的绳套，挥着金箍棒直打到城里，又一直打到森罗殿前，十位冥王见悟空长得的十分凶恶，吓得不知道该怎么办。悟空说∶“你们既然坐在王位上，就应该有点灵气，为什么不知道我来？俺老孙已经修成仙道，能长生不老。快拿生死簿来！”十位冥王赶快叫判官拿出生死本来查。 
+　　悟空登上森罗殿，一直查到魂字一千三百五十号，才找到了自己的名字，顺手拿起笔把所有猴子的名字通通勾掉，说∶“这下好极了，好极了，今后再也不归你们管了。”说完又一路打出了幽冥界。十位冥王赶忙到翠云宫去见地藏王菩萨，商量如何向玉皇大帝报告。
+
+  
+		
\ No newline at end of file
Index: .idea/.gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/.gitignore b/.idea/.gitignore
new file mode 100644
--- /dev/null	(date 1734766589031)
+++ b/.idea/.gitignore	(date 1734766589031)
@@ -0,0 +1,3 @@
+# 默认忽略的文件
+/shelf/
+/workspace.xml
Index: other/text/7.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/7.txt b/other/text/7.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/7.txt	(date 1734766590579)
@@ -0,0 +1,24 @@
+第7回 高老庄唐僧收八戒
+
+  　　这一天天快黑了，他们来到一个叫做高老庄的村子。碰巧，庄主高太公正在到处寻找能捉妖怪的法师。悟空一听非常高兴地说∶“不用找了，我就是专门捉妖怪的。” 
+　　原来，高太公有三个女儿，前两个女儿已经出嫁，到了三女儿，就想找个上门女婿来支撑门户。三年前来了个又黑又壮的青年，自称是福陵山人，姓猪，想到高家当女婿。三女儿对他还算满意，高太公就让他们成了家。 
+　　开始这个女婿很勤快，耕田下地，收割粮食，样样都行。没想到过了一阵，他突然变成一个猪头猪脑的妖怪，一顿饭要吃三五斗米，来去都腾云驾雾。这半年来，竟然把三女儿锁在后院，不让人进去。 
+　　悟空听了高太公的话，拍拍胸脯说∶“这个妖怪我捉定了，今天晚上就让他写退婚书，永远不再碰你女儿。”高太公问他要几个帮手，悟空说∶“一个也不要，只要把我师父照顾好就行了。”高太公连忙照办。 
+　　安顿好了师父，悟空让高太公带路来到后院。他打掉铁锁，走进院中一间黑洞洞的屋子。高太公和女儿见面，忍不住抱在一起痛哭起来。三女儿告诉他们∶“那妖怪知道我爹要请法师捉拿他，每天天一亮就走，晚上才回来。” 
+　　悟空让高太公父女离开，自己变成三女儿的模样。没过多久，院外一阵狂风刮来，那妖怪出现在半空中。悟空连忙向床上一靠，装出有病的样子，那妖怪摸进房中，口中喊着∶“姐姐，姐姐，你在哪儿呀？” 
+　　悟空故意叹口气说∶“我听爹今天在外面骂你，还说请了法师来抓你！ 
+　　”那妖怪说∶“不怕，不怕，咱们上床睡吧！”悟空说∶“我爹请的可是那五百年前大闹天宫的齐天大圣，你不害怕吗？”那妖怪倒吸了口凉气说∶“咱们做不成夫妻了。” 
+　　猪精打开门就往外跑，悟空从后面一把扯住他的后领子，把脸一抹，现出原形大叫道∶“泼怪，你看我是谁？”那妖怪一见是悟空，吓得手脚发麻，“呼”地一下化成一阵狂风跑了。 
+　　悟空跟着这股妖风一路追到高山上，只见那股妖风钻进了一个洞里。悟空刚落下云头，那妖怪已现形从洞中出来了，手里拿着一柄九齿钉耙骂道∶“你这个弼马温！当年大闹天宫，不知连累了我们多少人。今天又来欺负我，让你尝尝我的厉害，看耙！” 
+　　悟空举起棒架住了钉耙，问∶“怎么，你认识俺老孙！”那妖怪说出了自己的来历∶原来他是天上的天蓬元师，在王母娘娘的蟠桃会上喝得酩酊大醉，闯进了广寒宫，见嫦娥长得十分美丽，就上去调戏嫦娥。 
+　　玉皇大帝知道这件事后，要根据天条将他处死。多亏太白金星求情，才保住了性命，但要重打二千铜锤，并打入凡间投胎。没想到他急于投胎转世，竟错投了猪胎，落得如此模样。这时他和悟空打了一会儿，就觉得抵挡不住，拔腿就往洞中逃。 
+　　悟空站在洞口骂，那妖怪也不出来。悟空一见，气得乱蹦乱跳，拿起金箍棒打碎了洞门，那妖怪听见洞门被打碎的声音，只好跳出来骂道∶“我在高老庄招亲，跟你有什么关系，你欺人太甚，我这把钉耙绝不饶你！” 
+　　悟空想跟他玩玩，就站着不动，不管那妖怪怎么打，悟空的头皮连红都不红。那妖怪使劲一打，溅得火星乱飞，这下可把他吓坏了，说∶“好头！ 
+　　好头！你原来不是在花果山水帘洞，怎么跑到这儿来了，是不是我丈人到那儿把你请来的？” 
+　　悟空说∶“不是，是我自己改邪归正了，保护唐僧西天取经路过这…… 
+　　”妖怪一听“取经”二字，“啪”地一声一丢钉耙，拱了拱手说∶“麻烦你引见一下，我受观音菩萨劝导，她叫我在这里等你们，我愿意跟唐僧西天取经，也好将功折罪。” 
+　　两个人放火烧了云栈洞，悟空将妖怪的双手反绑上，押回高老庄。那妖怪“扑通”一声跪在唐僧面前，把观音菩萨劝他行善的事说了一遍。唐僧十分高兴，叫悟空给他松绑，又请高太公抬出香炉烛台，拜谢了观音，还给他取了个法号叫猪悟能，别名猪八戒。 
+　　高太公又给猪八戒准备了一套僧衣、僧鞋、僧帽等穿上。临走的时候，八戒一再叮嘱说∶“丈人啊！你好好照看我老婆，如果取不成经，我还是要还俗的。你不要把我的老婆再许给别人呀！”悟空听了笑骂他胡说八道，八戒却说∶“我这是给自己留条后路呢！”
+
+  
+		
\ No newline at end of file
Index: other/text/4.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/4.txt b/other/text/4.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/4.txt	(date 1734766590579)
@@ -0,0 +1,40 @@
+第4回 五行山从师取真经
+
+  　　五百年以后，观音菩萨奉了如来佛的法旨，带着锦袈裟等五件宝贝，跟惠岸行者一块儿来到东土大唐，寻找去西天求取三藏真经的人。师徒二人在半空中驾着云，来到大唐京城长安的上空。这时已是贞观十三年。 
+　　这一天，正是唐太宗李世民命令高僧陈玄奘在化生寺设坛宣讲佛法的日子。陈玄奘是如来佛二弟子金蝉子转世的，观音暗中选定他为取经人，自己与惠岸行者变成了游方和尚，捧着袈裟等宝贝到皇宫门外，要求拜见唐太宗，给他献宝。 
+　　唐太宗一向喜欢佛经，立即叫他们上殿，问那些宝贝一共要多少钱。观音说∶“如来佛祖那儿有三藏真经，你如果派陈玄奘去西天求取真经，这些宝贝就送给你了。”说完，跟惠岸行者变成原来的样子，驾起云走了。太宗一见是观音菩萨，连忙带领满朝文武官员向天朝拜。 
+　　唐太宗十分高兴，和陈玄奘结成了兄弟，要他去西天取经，将护身袈裟等宝物送给了他，并将他的名字改为“唐三藏”。过了几天，三藏要出发了，唐太宗便率领文武百官一路送到长安城外，和三藏依依惜别。 
+　　唐三藏别名唐僧。他和两个仆人赶了两天路，来到法门寺，寺里的和尚赶忙出来迎接。晚上，和尚们坐在一起议论去西天取经的路途艰险，唐僧用手指着心口说∶“只要有坚定的信念，那么任何危险都算不了什么！”和尚们连声称赞。 
+　　第二天，唐僧主仆含泪辞别了和尚，又骑着马继续向西走去。不几天，就来到了大唐的边界河州，镇边总兵和本地的和尚道士把唐僧主仆接到福原寺休息。 
+　　第二天天还没亮，唐僧就把两个仆人叫了起来，三人借着月光赶路。走了十几里就开始上山了，道路起伏不平，杂草丛生，十分难走。他们只好一边拔草一边走。忽然一脚踏空，三人和马还一起摔进了深坑。主仆三人正在惊慌之时，忽然听见“抓起来！抓起来！”的叫喊声。 
+　　随着一阵狂风，出现了一群妖怪，抓住了主仆三人。唐僧偷偷看了看，上面坐着一个长相凶恶的魔王，那魔王一声令下，妖怪们把唐僧主仆绑了起来。这时一个小妖来报∶“熊山君和特处士到！” 
+　　魔王赶忙出去迎接，那两人称魔王为寅将军。寅将军打算用唐僧等人招待他的客人。熊山君说∶“今天，就选吃两个算了。”于是，寅将军把唐僧的两个仆人剖腹挖心，活活地吃掉了。唐僧差点被吓昏过去。 
+　　天快亮了，妖怪们都躲了起来。唐僧吓傻了，昏昏沉沉地睡着。忽然一个柱拐杖的老人慢慢向他走来，把手一挥，捆绑唐僧的绳子都断了，又向他吹一口气，唐僧醒了过来，连忙躬身施礼感谢老人，老人说∶“这个地方叫双叉岭，是个危险的地方。” 
+　　老人让唐僧拿上包袱，牵着马，把他领到大路上来。唐僧连忙拴好马，准备感谢，抬头一看，老人已乘着一只红顶白鹤飞走了，从空中掉下一张纸条，唐僧接过一看，才知老人就是太白金星，于是赶忙向空中不停地施礼。 
+　　唐僧骑着马，沿着山路往前走，走了半天，也不见一个人。他又渴又饿，想找点水喝。忽然看见前面有两只凶恶的老虎，张开了血盆大嘴，又往四周看看，发现身后是吐着红信的毒蛇，左边是有毒的虫子，右边又是些从未见过的野兽。唐僧被困在中间，急得不知如何是好，只好听天由命了。 
+　　就在这危急关头，野兽忽然都逃跑了。唐僧惊奇地四处观看，只见一个手拿钢叉，腰挂弓箭的大汉从山坡上走了过来。唐僧连忙跪下，合掌高叫∶“大王救命！”那大汉挽起唐僧说∶“我哪里是什么大王，只不过是一个猎户，叫刘伯钦。” 
+　　刘伯钦请唐僧到家中作客，唐僧非常高兴，牵着马，来到了刘伯钦的家。第二天，唐僧要上路了，刘伯钦按照母亲的意思，带了几个人，拿着捕猎的工具，要送一送唐僧。走了半天，他们来到一座大山前。 
+　　他们走到半山腰，刘伯钦等人站住说∶“长老，前面就要到两界山了，山东边归大唐管，山西边是鞑靼的疆域，我们是不能过去的，您自己走吧，一路上可要多多小心啊！”唐僧只好和他们道别，忽听山脚下有人大喊∶“师父快过来，师父快过来！” 
+　　唐僧吓得胆战心惊。刘伯钦赶忙说∶“长老莫怕，听老人说，当年王莽造反的时候，这座山从天而降，山下还压着一个饿不死，冻不坏的神猴，刚才肯定是那个神猴在叫喊，长老不妨过去看看。” 
+　　这神猴正是当年被如来压在山下的孙悟空，他一见唐僧就喊道∶“师父快救我出去，我保护你到西天取经。几天前观音菩萨来劝过我，让我给您当徒弟。”唐僧听了非常高兴，可是又很发愁，没有办法把孙悟空救出来。 
+　　孙悟空说只要把山顶上如来佛的金字压帖拿掉就行了。唐僧拿掉了金字压帖后，按照悟空的要求和刘伯钦等人退到十里之外的地方等着。忽然一声天崩地裂般的巨响，五行山裂成两半，顿时飞沙走石，满天灰尘，让人睁不开眼睛。 
+　　等到唐僧睁开眼睛时，悟空已经跪在地上，给他叩头。唐僧见他赤身裸体，就从包袱里拿出一双鞋和一条裤子让他穿上。刘伯钦见唐僧收了徒弟，非常高兴，告别了唐僧师徒回家去了。悟空立刻收拾行李，和师父一道出发。 
+　　没过多久，师徒二人出了大唐边界。忽然从草丛中跳出一只大老虎。孙悟空赶忙放下行李，从耳朵中取出金箍棒，高兴地说∶“老孙已经五百多年没有用过这宝贝了，今天用它弄件衣服穿穿！”说完抡起金箍棒对着老虎狠命一击，老虎当场就死了。 
+　　唐僧见了，惊得连嘴都合不住。悟空拔了根毫毛，变成一把尖刀，剥了虎皮，做了条皮裙围在腰间，然后，恭恭敬敬地扶唐僧上马，师徒继续赶路。忽然一声口哨声，跳出六个强盗，要抢他们的马和行李。 
+　　悟空放下行李，笑着说∶“我原来也是做山大王的，把你们抢的金银珠宝分我一半吧！”强盗们一听，气得头发都竖了起来，拿着刀枪就往悟空头上砍，可是乒乒乓乓砍了七、八十下，也没伤着悟空半根毫毛。 
+　　悟空见他们打累了，高喊一声∶“该俺老孙玩玩了！”他取出金箍棒一个个打，六个强盗就变成了肉酱。唐僧见了很不高兴地说∶“他们虽然是强盗，但也不至于都要打死，你这样残忍，怎能去西天取经呢？阿弥陀佛。” 
+　　孙悟空最受不了别人的气，他听师父这样一说，压不住心中的怒火，高声说到∶“既然师父这样说，那我就不去西天取经了，你自己去吧！老孙我可要回花果山了！”说完纵身一跳，驾上筋斗云，往东飞去了，等到唐僧抬起头，已经看不见孙悟空了。 
+　　唐僧没有办法，只好把行李放在马背上，一手拄着锡杖，一手牵着马，慢慢地往西走去，不久，就见对面来了位老妇人，手里捧着一件衣服和一顶花帽。唐僧赶忙牵住马，双手合掌，让路给老妇人过。 
+　　那老妇人走到唐僧跟前说道∶“你从哪里来呀，怎么一个人在山中走呢？”唐僧就把悟空不听话的事告诉了老妇人，老妇人听后微微一笑，说∶“我送你一件衣服和一顶花帽，给你那不听话的徒弟穿上吧！” 
+　　唐僧苦笑着说∶“唉，徒弟已经走了！要这些还有什么用呢？”老妇人笑着说∶“别急，徒弟我会帮你找回来的。我这儿呀，还有一篇咒语，叫做紧箍咒，你要牢牢记在心里，你让你的徒弟穿上这衣服，戴上帽子，他如果再不听话，你就念咒，他就不敢不听了！” 
+　　唐僧学会了紧箍咒，低头拜谢老妇人。这时老妇人已经变成一道金光，向东飞去。唐僧抬头一看，原来是观音菩萨，赶忙跪下叩头，然后把衣帽收到包袱里，坐在路边，加紧背诵紧箍咒，直到背得滚瓜烂熟。 
+　　观音菩萨驾着祥云，没走多远，碰上了从东边走过来的孙悟空。原来悟空离开唐僧之后，在东海龙王那儿吃了顿饭，在龙王的苦苦劝告之下，已回心转意。观音菩萨让他赶快回到唐僧身边，悟空二话不说，告别观音菩萨去追赶唐僧了。 
+　　见到唐僧，悟空把去龙王那儿吃饭的事情说了一遍，又问∶“师父，你也饿了吧！我去化些斋饭来。”唐僧摇摇头说∶“不用了，包袱里还有些干粮，你给师父拿来吧！”悟空打开包袱，发现观音菩萨给的衣帽十分漂亮，便向唐僧讨取。 
+　　唐僧点头答应了。悟空高兴得抓耳挠腮，忙穿上了衣服，戴上了帽子。 
+　　唐僧要试试紧箍咒灵不灵，就小声念了起来，悟空马上痛得满地打滚，拼命去扯那帽子，可帽子却像长在肉里一样，取也取不下来，扯也扯不烂。 
+　　悟空发现头痛是因为师父在念咒，嘴里喊着“师父别念了！别念了！” 
+　　暗地里取出金箍棒，想把唐僧一棒打死。唐僧见了，紧箍咒越念越快，悟空的头越来越疼，没有办法，只好跪地求饶∶“师父，是我错了，徒儿知道错了，不要再念咒了吧！” 
+　　唐僧见他已经知错，就住了口。悟空的头马上就不痛了，他想这咒语一定是观音菩萨教的，就吵着要去南海找观音菩萨算帐。唐僧说∶“她既然能教我这紧箍咒，肯定也会念咒！”悟空猛吸了一口气，不再胡来，发誓以后一定听师父的话，保护唐僧西天取经。
+
+  
+		
\ No newline at end of file
Index: other/text/5.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/5.txt b/other/text/5.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/5.txt	(date 1734766590579)
@@ -0,0 +1,14 @@
+第5回 应愁涧白龙马收缰
+
+  　　师徒俩继续向西行。一天，他们来到蛇盘山鹰愁涧，突然从涧中钻出一条白龙来，张着爪子向唐僧冲了过来，悟空慌忙背起唐僧，驾云就跑。那龙追不上悟空，就张开大嘴把白马给吞吃了，然后又钻进深涧了。 
+　　悟空把师父安顿在一个安全地方。转身回到涧边去牵马拿行李，发现马不见了，想着一定是被白龙吃了，就在涧边破口大骂∶“烂泥鳅，把我的马吐出来！”白龙听见有人骂他，气得眼睛都红了，跳出水面，张牙舞爪地向悟空扑来。 
+　　那龙根本不是悟空的对手，几个回合就累得浑身是汗，转身就逃到水里。悟空又骂了一阵，不见白龙出来，便使了个翻江倒海的本领，把这个清澈的涧水弄得泥沙翻滚，浑浊不清。 
+　　那龙在水里待不住了，就硬着头皮跳出来，和悟空打了起来，双方战了几十个回合，白龙实在打不过，摇身变成一条水蛇，钻进了草丛。悟空赶忙追过去，可是连蛇的影子都找不到，气得他把牙咬得乱响。 
+　　于是，悟空念咒语，把山神和土地都叫了出来，问他们白龙从哪里来的。山神和土地小心翼翼地说∶“这白龙是观音菩萨放在这儿等候你们，和你们一起取经的。”悟空一听，气得要找观音菩萨讲道理。 
+　　观音菩萨料事如神，驾云来到鹰愁涧，告诉悟空∶“这白龙原是西海龙王的儿子，犯了死罪，是我讲了个人情，让他给唐僧当马骑的。如果没这匹龙马，你们就去不了西天。”悟空急着说∶“他藏在水里不出来，怎么办？ 
+　　” 
+　　观音菩萨面带微笑，朝涧中喊了一声，那白龙立刻变成一个英俊的公子，来到菩萨跟前。菩萨说∶“小白龙，你师父已经来了！”边说边解下白龙脖上的夜明珠，用柳条蘸些甘露向他身上一挥，吹了口仙气，喊声“变”，白龙就变成了一匹白马。 
+　　观音菩萨叫悟空牵着白马去见唐僧，自己回南海落伽山去了。悟空牵着马，兴高采烈地来到唐僧跟前。唐僧一边用手摸着马头，一边说∶“好马，好马，你是在哪儿找的马？”悟空把经过说了一遍，唐僧连忙向南磕头，感谢观音菩萨。
+
+  
+		
\ No newline at end of file
Index: other/text/6.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/6.txt b/other/text/6.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/6.txt	(date 1734766590579)
@@ -0,0 +1,34 @@
+第6回 观音院斗宝失袈裟
+
+  　　唐僧骑上白龙马，走起路来就轻松了许多。一天傍晚，师徒二人来到山谷里的一座观音院。门口的和尚一听是大唐来的高僧，要到西天去取经，连忙施礼，恭恭敬敬地请他们进院子休息。 
+　　唐僧师徒刚刚坐好，两名小和尚搀扶着一个驼背的和尚，慢慢地走了进来。唐僧连忙起身，双手合掌，施礼相迎。老和尚一边还礼，一边叫人端茶来。不一会儿，两个小童端着精美的茶具进来了。 
+　　唐僧喝了一口茶，夸了几句这茶具。老和尚很高兴，然后卖弄地讲起了茶经，接着又问唐僧有什么东土大唐带来的宝贝，拿出来看一看。悟空见老和尚这般卖弄，心中早有一百个不服气了，不等师父说话，便抢着说∶“师父，把你的袈裟让他们见识见识！” 
+　　老和尚一听袈裟，更是得意，大笑起来，让人拿出十二只箱子，将里面的袈裟全部抖了出来，竟有上百件，而且每一件都很漂亮。悟空见了，也不言语，拿出了唐僧的袈裟抖开，顿时满屋金光四射，让人睁不开眼睛。 
+　　老和尚看呆了，一条毒计爬上心头，找了个借口，请求唐僧把袈裟借给他仔细看上一晚，明早奉还。唐僧还未开口，悟空抢先说∶“就借给他看一晚吧！不会有事的！”唐僧想要阻止已经来不及了，只好很不情愿地把袈裟借给老和尚。 
+　　晚上，老和尚偷偷让小和尚搬来许多木柴，想把唐僧师徒烧死。悟空听到院子里很吵，觉得奇怪，害怕师父被惊醒，就变成一个小蜜蜂，飞到院中，看到眼前的情景，觉得很可笑，眼珠一转，想出了一条妙计。 
+　　悟空驾起筋斗云，来到南天门，守门的天兵天将见是大闹天宫的齐天大圣来了，吓得乱成一团。悟空高叫∶“别怕！别怕！我不是来打架的，是来找广目天王借避火罩，去救师父的！”广目天王只好将宝贝借给悟空。 
+　　悟空拿着避火罩回到观音院，把师父的禅房罩住，然后悠闲地坐在屋顶，看和尚们放火。刹那间，大火熊熊燃烧。悟空想，这些和尚也太狠了，就吹了一口气，立刻刮起一阵狂风，火借风势，整个观音院顿时变成了一片火海。 
+　　这场大火引来了一个妖怪。原来这座观音院的南面有座黑风山，山中黑风洞里住着一个黑风怪。他远远地看见寺庙起火，就想着趁火打劫偷点东西，于是驾云飘进方丈房中，看见桌上的包袱放出金光，打开一看，竟是件价值连城的袈裟。 
+　　黑风怪偷了那件袈裟，驾云回到洞中。悟空只管坐在屋顶吹火，却没注意到黑风怪。天快亮时，悟空见火已经快灭了，才收起避火罩，还给了广目天王。回到禅房，见师父还在熟睡，就轻轻地叫醒了师父。 
+　　唐僧打开房门，见院中四处都是乌黑烧焦的木头，好端端的观音院已经不存在了，感到非常吃惊，悟空就把昨晚发生的事说了一遍。唐僧心中想着袈裟，就和悟空一块去找，寺里的和尚看见他们，还以为是冤魂来了，吓得连连跪地求饶。 
+　　那驼背老和尚看见寺院被烧，又不见了袈裟，正生气，又听唐僧没有烧死，来取袈裟了，吓得不知怎么办才好。最后一狠心，一头往墙上撞去，顿时血流如注，当场就死了。唐僧知道后，埋怨悟空说“唉！徒儿，你何苦要和别人斗气比阔呢？现在可怎么办呀！” 
+　　悟空手拿金箍棒，追问那些和尚袈裟在哪里，和尚都说不知道。悟空想了又想问道∶“这附近可有妖怪？”和尚都说黑风山上有个黑风怪。悟空板着脸说∶“好好侍候我师父，如有不周，小心脑袋！”说着一棒打断了一堵墙。 
+　　悟空一个筋斗来到黑风山，按落云头，往林中走去。忽听坡前有人在说笑，悟空侧身躲在岩石后面，偷偷望去，见地上坐着三个妖魔，为首的一个黑脸大汉说∶“昨天晚上有缘得到了一件佛衣，特地请二位来，开个佛衣盛会！” 
+　　悟空听得一清二楚，一边骂着∶“偷东西的坏家伙！”一边跳上前去，“呼”的就是一捧。黑脸大汉就是黑风怪，变成一股风逃走了；还有个道士也跑了，只有那个白衣秀才没来得及逃走，被悟空一棒打死，现出原形，原来是条大白花蛇。 
+　　悟空紧跟那股风来到一座山峰上，远远地看见对面山崖上有一座洞府，门前有一石碑，上面写着∶“黑风山黑风洞”几个大字。悟空来到洞前，用棒子敲着门，高声叫到∶“坏家伙，还我袈裟来！”小妖怪看到悟空气势汹汹，连忙跑进去报告黑风怪。 
+　　黑风怪刚才在山坡逃走是因为没带武器，现在是在他的地盘上，他可不怕。他穿上乌金甲，提着黑缨枪，出洞和悟空打了起来。打到中午，黑风怪说要吃饭，饭后再打。悟空也不说话，只是打，黑风怪只得再变成一股清风逃回洞中。 
+　　不管悟空在洞外骂得有多难听，黑风怪就是不出来。悟空急得没有办法，只得先回观音院去看师父了。回到院中，随便吃了些东西，又驾云来到黑风山，看见一个小妖拿着一个装请柬的木匣急急忙忙向前走，就一棒把它打死了。 
+　　悟空打开木匣一看，里面装的竟是黑风怪邀请观音院那老和尚的请柬，这才明白，老和尚早就和妖怪有来往，悟空眼珠一转，心生一条妙计，马上变成了老和尚的模样，摇摇摆摆地走到洞口，小妖一见是熟人，连忙开门相迎。 
+　　黑风怪没有看出什么破绽，扶着老和尚走进中厅，还没说几句话，在外面巡逻的小妖进来报告说送信的小妖已经被打死了。黑风怪立刻就明白了是怎么回事，拿出枪来狠狠刺向悟空，悟空侧身躲开，嘿嘿笑了几声，露出了本来面目，和妖怪打了起来。 
+　　两人你一枪，我一棒，打得难分难解，一直到太阳落山。那妖怪说∶“现在天快要黑了，明天再和你打！”悟空知道这家伙又要逃跑，哪肯放过，当头一棒打去，那妖怪化作一股清风，溜回洞中去了。 
+　　悟空没有办法，只好回到观音院。唐僧看到袈裟还没有夺回来，心中非常着急。晚上怎么也睡不着。第二天天刚亮，悟空对唐僧说∶“师父请放心，老孙今天要是夺不回袈裟，就不回来见你！”原来他已决定找观音菩萨想办法。 
+　　悟空驾云来到南海落伽山，见到观音菩萨，上前深深鞠了一躬，说明来意。观音菩萨听后叹了口气说∶“你这猴子，不该当众卖弄宝衣，更不该放火烧了寺院弄成现在这个样子。”说完，嘱咐了童子几句，和悟空驾着云，飞往黑风山。 
+　　他们很快来到黑风山，远远看见那天在山坡前的道士端着玉盘走了过来。悟空上前一棒打死了道士，现出了原形，原来是只大灰狼。悟空捡起盘子，看见里面有两粒仙丹，原来他是去参加佛衣盛会的。 
+　　悟空灵机一动，想出一条妙计，他让观音菩萨变成那道士，自己则变成一颗仙丹，只不过比原来的大一些。观音菩萨把他放在盘中，向洞中走去，按悟空说的计策，要让黑风怪吃下那颗仙丹。 
+　　观音菩萨来到洞中，把仙丹送到黑风怪手中，说∶“小道献上一颗仙丹，祝大王健康长寿！”黑风怪十分高兴，接过仙丹刚送到嘴边，没想到仙丹自动滑了下去。 
+　　悟空一到黑风怪的肚子里，就恢复了原形，在里面打起了猴拳，黑风怪痛得在地上直打滚。观音菩萨也恢复了原形，命令他交出佛衣，黑风怪痛得受不了了，让小妖拿来袈裟。观音菩萨接过佛衣，拿出一个小金圈儿，套在黑风怪头上。 
+　　观音这才让悟空出来。悟空刚从黑风怪的鼻孔里跳出来，黑风怪就摆出一副凶相，拿着黑缨枪向观音刺去。观音浮在空中，念动咒语，黑风怪马上头痛了起来，只好跪在地上，求观音饶命，并说自己愿意出家。 
+　　观音菩萨把佛衣交给悟空，带着黑风怪回南海去了。悟空见黑风洞中的小妖早已逃离，就放了一把火把洞烧了，然后驾云赶回观音院。唐僧和寺里的和尚们看见悟空取回了袈裟，都很高兴。第二天，唐僧师徒离开了观音院，又向西出发
+
+  
+		
\ No newline at end of file
Index: other/text/1.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/other/text/1.txt b/other/text/1.txt
new file mode 100644
--- /dev/null	(date 1734766590579)
+++ b/other/text/1.txt	(date 1734766590579)
@@ -0,0 +1,32 @@
+第1回 惊天地美猴王出世
+
+  　　这是一个神话故事，传说在很久很久以前，天下分为东胜神洲、西牛贺洲、南赡部洲、北俱芦洲。在东胜神洲傲来国，有一座花果山，山上有一块仙石，一天仙石崩裂，从石头中滚出一个卵，这个卵一见风就变成一个石猴，猴眼射出一道道金光，向四方朝拜。
+　　那猴能走、能跑，渴了就喝些山涧中的泉水，饿了就吃些山上的果子。
+　　整天和山中的动物一起玩乐，过得十分快活。一天，天气特别热，猴子们为了躲避炎热的天气，跑到山涧里洗澡。它们看见这泉水哗哗地流，就顺着涧往前走，去寻找它的源头。
+　　猴子们爬呀、爬呀，走到了尽头，却看见一股瀑布，像是从天而降一样。猴子们觉得惊奇，商量说∶“哪个敢钻进瀑布，把泉水的源头找出来，又不伤身体，就拜他为王。”连喊了三遍，那石猴呼地跳了出来，高声喊道∶“我进去，我进去！”
+　　那石猴闭眼纵身跳入瀑布，觉得不像是在水中，这才睁开眼，四处打量，发现自己站在一座铁板桥上，桥下的水冲贯于石窍之间，倒挂着流出来，将桥门遮住，使外面的人看不到里面。石猴走过桥，发现这真是个好地方，石椅、石床、石盆、石碗，样样都有。
+　　这里就像不久以前有人住过一样，天然的房子，安静整洁，锅、碗、瓢、盆，整齐地放在炉灶上。正当中有一块石碑，上面刻着∶花果山福地，水帘洞洞天。石猴高兴得不得了，忙转身向外走去，嗖的一下跳出了洞。
+　　猴子们见石猴出来了，身上又一点伤也没有，又惊又喜，把他团团围住，争著问他里面的情况。石猴抓抓腮，挠挠痒，笑嘻嘻地对大家说∶“里面没有水，是一个安身的好地方，刮大风我们有地方躲，下大雨我们也不怕淋。”猴子们一听，一个个高兴得又蹦又跳。
+　　猴子们随着石猴穿过了瀑布，进入水帘洞中，看见了这么多的好东西，一个个你争我夺，拿盆的拿盆，拿碗的拿碗，占灶的占灶，争床的争床，搬过来，移过去，直到精疲力尽为止。猴子们都遵照诺言，拜石猴为王，石猴从此登上王位，将石字省去，自称“美猴王”。
+　　美猴王每天带着猴子们游山玩水，很快三、五百年过去了。一天正在玩乐时，美猴王想到自己将来难免一死，不由悲伤得掉下眼泪来，这时猴群中跳出个通背猿猴来，说∶“大王想要长生不老，只有去学佛、学仙、学神之术。”
+　　美猴王决定走遍天涯海角，也要找到神仙，学那长生不老的本领。第二天，猴子们为他做了一个木筏，又准备了一些野果，于是美猴王告别了群猴们，一个人撑着木筏，奔向汪洋大海。
+　　大概是美猴王的运气好，连日的东南风，将他送到西北岸边。他下了木筏，登上了岸，看见岸边有许多人都在干活，有的捉鱼，有的打天上的大雁，有的挖蛤蜊，有的淘盐，他悄悄地走过去，没想到，吓得那些人将东西一扔，四处逃命。
+　　这一天，他来到一座高山前，突然从半山腰的树林里传出一阵美妙的歌声，唱的是一些关于成仙的话。猴王想∶这个唱歌的人一定是神仙，就顺着歌声找去。
+　　唱歌的是一个正在树林里砍柴的青年人，猴王从这青年人的口中了解到，这座山叫灵台方寸山，离这儿七八里路，有个斜月三星洞，洞中住着一个称为菩提祖师的神仙。
+　　美猴王告别打柴的青年人，出了树林，走过山坡，果然远远地看见一座洞府，只见洞门紧紧地闭着，洞门对面的山岗上立着一块石碑，大约有三丈多高，八尺多宽，上面写着十个大字∶“灵台方寸山斜月三星洞”。正在看时，门却忽然打开了，走出来一个仙童。
+　　美猴王赶快走上前，深深地鞠了一个躬，说明来意，那仙童说∶“我师父刚才正要讲道，忽然叫我出来开门，说外面来了个拜师学艺的，原来就是你呀！跟我来吧！”美猴王赶紧整整衣服，恭恭敬敬地跟着仙童进到洞内，来到祖师讲道的法台跟前。
+　　猴王看见菩提祖师端端正正地坐在台上，台下两边站着三十多个仙童，就赶紧跪下叩头。祖师问清楚他的来意，很高兴，见他没有姓名，便说∶“你就叫悟空吧！”
+　　祖师叫孙悟空又拜见了各位师兄，并给悟空找了间空房住下。从此悟空跟着师兄学习生活常识，讲究经典，写字烧香，空时做些扫地挑水的活。
+　　很快七年过去了，一天，祖师讲道结束后，问悟空想学什么本领。孙悟空不管祖师讲什么求神拜佛、打坐修行，只要一听不能长生不老，就不愿意学，菩提祖师对此非常生气。
+　　祖师从高台上跳了下来，手里拿着戒尺指着孙悟空说∶“你这猴子，这也不学，那也不学，你要学些什么？”说完走过去在悟空头上打了三下，倒背着手走到里间，关上了门。师兄们看到师父生气了，感到很害怕，纷纷责怪孙悟空。
+　　孙悟空既不怕，又不生气，心里反而十分高兴。当天晚上，悟空假装睡着了，可是一到半夜，就悄悄起来，从前门出去，等到三更，绕到后门口，看见门半开半闭，高兴地不得了，心想∶“哈哈，我没有猜错师父的意思。”
+　　孙悟空走了进去，看见祖师面朝里睡着，就跪在床前说∶“师父，我跪在这里等着您呢！”祖师听见声音就起来了，盘着腿坐好后，严厉地问孙悟空来做什么，悟空说∶“师父白天当着大家的面不是答应我，让我三更时从后门进来，教我长生不老的法术吗？”
+　　菩提祖师听到这话心里很高兴。心想∶“这个猴子果然是天地生成的，不然，怎么能猜透我的暗谜。”于是，让孙悟空跪在床前，教给他长生不老的法术。孙悟空洗耳恭听，用心理解，牢牢记住口诀，并叩头拜谢了祖师的恩情。
+　　很快三年又过去了，祖师又教了孙悟空七十二般变化的法术和驾筋斗云的本领，学会了这个本领，一个筋斗便能翻出十万八千里路程。孙悟空是个猴子，本来就喜欢蹦蹦跳跳的，所以学起筋斗云来很容易。
+　　有一个夏天，孙悟空和师兄们在洞门前玩耍，大家要孙悟空变个东西看看，孙悟空心里感到很高兴，得意地念起咒语，摇身一变变成了一棵大树。
+　　师兄们见了，鼓着掌称赞他。
+　　大家的吵闹声，让菩提祖师听到了，他拄着拐杖出来，问∶“是谁在吵闹？你们这样大吵大叫的，哪里像个出家修行的人呢？”大家都赶紧停住了笑，孙悟空也恢复了原样，给师父解释，请求原谅。
+　　菩提祖师看见孙悟空刚刚学会了一些本领就卖弄起来，十分生气。祖师叫其他人离开，把悟空狠狠地教训了一顿，并且要把孙悟空赶走。孙悟空着急了，哀求祖师不要赶他走，祖师却不肯留下他，并要他立下誓言∶任何时候都不能说孙悟空是菩提祖师的徒弟。
+
+  
+		
\ No newline at end of file
